{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Phishing Dataset - Data Preprocessing\n",
    "\n",
    "This notebook performs data cleaning and preprocessing for the BERT-LSTM phishing detection model.\n",
    "\n",
    "## Steps:\n",
    "1. Load all datasets\n",
    "2. Filter out garbage labels (keep only 0 and 1)\n",
    "3. Standardize schema with metadata fields (sender, receiver, date, urls)\n",
    "4. Combine all clean datasets\n",
    "5. Text preprocessing and cleaning\n",
    "6. Train/validation/test split\n",
    "7. Save processed data\n",
    "\n",
    "## Final Schema:\n",
    "- **sender** - Email sender (null for datasets without metadata)\n",
    "- **receiver** - Email receiver (null for datasets without metadata)\n",
    "- **date** - Email date (null for datasets without metadata)\n",
    "- **text** - Combined subject + body\n",
    "- **urls** - URL information (null for datasets without metadata)\n",
    "- **label** - 0 (legitimate) or 1 (phishing)\n",
    "- **source** - Original dataset name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_PATH = Path('../../../data/unprocessed/email-detection/Seven Emails phishing dataset')\n",
    "PROCESSED_PATH = Path('../../../data/processed/email-detection')\n",
    "\n",
    "# Create processed data directory\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Raw data path: {DATA_PATH}')\n",
    "print(f'Processed data path: {PROCESSED_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, file_name):\n",
    "    \"\"\"Load dataset with appropriate error handling\"\"\"\n",
    "    try:\n",
    "        if file_name in ['TREC-05.csv', 'TREC-06.csv']:\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                on_bad_lines='skip',\n",
    "                engine='python',\n",
    "                encoding='utf-8',\n",
    "                quoting=1\n",
    "            )\n",
    "            return df, 'loaded_with_skipped_lines'\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "            return df, 'loaded_successfully'\n",
    "    except Exception as e:\n",
    "        return None, f'error: {str(e)}'\n",
    "\n",
    "csv_files = ['Assassin.csv', 'CEAS-08.csv', 'Enron.csv', 'Ling.csv', 'TREC-05.csv', 'TREC-06.csv', 'TREC-07.csv']\n",
    "datasets = {}\n",
    "\n",
    "print('Loading datasets...')\n",
    "print('='*70)\n",
    "for file in csv_files:\n",
    "    df, status = load_dataset(DATA_PATH / file, file)\n",
    "    if df is not None:\n",
    "        name = file.replace('.csv', '')\n",
    "        datasets[name] = df\n",
    "        print(f'✓ {file:20s} - {len(df):>7,} rows - {status}')\n",
    "    else:\n",
    "        print(f'✗ {file:20s} - {status}')\n",
    "\n",
    "print(f'\\nTotal datasets loaded: {len(datasets)}')\n",
    "print(f'Total raw emails: {sum(len(df) for df in datasets.values()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter and Clean Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_labels(df, dataset_name):\n",
    "    \"\"\"Filter dataset to keep only valid binary labels (0 and 1)\"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Convert labels to numeric, coerce errors to NaN\n",
    "    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
    "    \n",
    "    # Filter to keep only 0 and 1\n",
    "    df_clean = df[df['label'].isin([0, 1])].copy()\n",
    "    \n",
    "    # Convert to integer\n",
    "    df_clean['label'] = df_clean['label'].astype(int)\n",
    "    \n",
    "    removed = original_count - len(df_clean)\n",
    "    \n",
    "    print(f'{dataset_name:15s} - Original: {original_count:>6,} | Clean: {len(df_clean):>6,} | Removed: {removed:>5,} ({removed/original_count*100:>5.2f}%)')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "print('Cleaning labels (keeping only 0 and 1)...')\n",
    "print('='*70)\n",
    "\n",
    "cleaned_datasets = {}\n",
    "for name, df in datasets.items():\n",
    "    cleaned_datasets[name] = clean_labels(df, name)\n",
    "\n",
    "total_clean = sum(len(df) for df in cleaned_datasets.values())\n",
    "total_removed = sum(len(datasets[name]) - len(cleaned_datasets[name]) for name in datasets.keys())\n",
    "\n",
    "print('='*70)\n",
    "print(f'Total clean emails: {total_clean:,}')\n",
    "print(f'Total removed: {total_removed:,}')\n",
    "print(f'Retention rate: {total_clean/(total_clean+total_removed)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Standardize Schema and Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_schema(df, dataset_name):\n",
    "    \"\"\"Standardize schema to include metadata fields: sender, receiver, date, subject, body, urls, label, source\"\"\"\n",
    "    \n",
    "    # Ensure subject and body exist\n",
    "    if 'subject' not in df.columns or 'body' not in df.columns:\n",
    "        print(f'Warning: {dataset_name} missing subject or body columns')\n",
    "        return None\n",
    "    \n",
    "    # Start with required columns\n",
    "    df_std = pd.DataFrame()\n",
    "    \n",
    "    # Add metadata columns (if they exist, otherwise add as None)\n",
    "    df_std['sender'] = df['sender'] if 'sender' in df.columns else None\n",
    "    df_std['receiver'] = df['receiver'] if 'receiver' in df.columns else None\n",
    "    df_std['date'] = df['date'] if 'date' in df.columns else None\n",
    "    df_std['urls'] = df['urls'] if 'urls' in df.columns else None\n",
    "    \n",
    "    # Add text columns\n",
    "    df_std['subject'] = df['subject']\n",
    "    df_std['body'] = df['body']\n",
    "    \n",
    "    # Add label and source\n",
    "    df_std['label'] = df['label']\n",
    "    df_std['source'] = dataset_name\n",
    "    \n",
    "    return df_std\n",
    "\n",
    "print('Standardizing schemas...')\n",
    "print('='*70)\n",
    "\n",
    "standardized_datasets = []\n",
    "for name, df in cleaned_datasets.items():\n",
    "    df_std = standardize_schema(df, name)\n",
    "    if df_std is not None:\n",
    "        standardized_datasets.append(df_std)\n",
    "        has_metadata = 'sender' in df.columns and 'urls' in df.columns\n",
    "        schema_type = 'Full (with metadata)' if has_metadata else 'Minimal (text only)'\n",
    "        print(f'✓ {name:15s} - {len(df_std):>6,} emails - {schema_type}')\n",
    "\n",
    "# Combine all datasets\n",
    "print('\\nCombining datasets...')\n",
    "df_combined = pd.concat(standardized_datasets, ignore_index=True)\n",
    "\n",
    "print(f'\\nCombined dataset shape: {df_combined.shape}')\n",
    "print(f'Columns: {list(df_combined.columns)}')\n",
    "\n",
    "# Check metadata availability\n",
    "print(f'\\nMetadata availability:')\n",
    "sender_count = df_combined['sender'].notna().sum()\n",
    "receiver_count = df_combined['receiver'].notna().sum()\n",
    "date_count = df_combined['date'].notna().sum()\n",
    "urls_count = df_combined['urls'].notna().sum()\n",
    "total_count = len(df_combined)\n",
    "\n",
    "sender_pct = sender_count / total_count * 100\n",
    "receiver_pct = receiver_count / total_count * 100\n",
    "date_pct = date_count / total_count * 100\n",
    "urls_pct = urls_count / total_count * 100\n",
    "\n",
    "print(f'  Emails with sender: {sender_count:,} ({sender_pct:.1f}%)')\n",
    "print(f'  Emails with receiver: {receiver_count:,} ({receiver_pct:.1f}%)')\n",
    "print(f'  Emails with date: {date_count:,} ({date_pct:.1f}%)')\n",
    "print(f'  Emails with urls: {urls_count:,} ({urls_pct:.1f}%)')\n",
    "\n",
    "print(f'\\nLabel distribution:')\n",
    "print(df_combined['label'].value_counts().sort_index())\n",
    "print(f'\\nSource distribution:')\n",
    "print(df_combined['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values before cleaning:')\n",
    "print('='*70)\n",
    "print(df_combined.isnull().sum())\n",
    "print(f'\\nRows with missing subject: {df_combined[\"subject\"].isnull().sum()}')\n",
    "print(f'Rows with missing body: {df_combined[\"body\"].isnull().sum()}')\n",
    "\n",
    "# Fill missing values with empty string\n",
    "df_combined['subject'] = df_combined['subject'].fillna('')\n",
    "df_combined['body'] = df_combined['body'].fillna('')\n",
    "\n",
    "# Drop rows where both subject and body are empty\n",
    "before_drop = len(df_combined)\n",
    "df_combined = df_combined[\n",
    "    (df_combined['subject'].astype(str).str.strip() != '') | \n",
    "    (df_combined['body'].astype(str).str.strip() != '')\n",
    "].copy()\n",
    "after_drop = len(df_combined)\n",
    "\n",
    "print(f'\\nRows dropped (empty subject AND body): {before_drop - after_drop}')\n",
    "print(f'Remaining rows: {after_drop:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove very long sequences of repeated characters (likely noise)\n",
    "    text = re.sub(r'(.)\\1{10,}', r'\\1\\1\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "print('Preprocessing text...')\n",
    "print('='*70)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_combined['subject_clean'] = df_combined['subject'].apply(preprocess_text)\n",
    "df_combined['body_clean'] = df_combined['body'].apply(preprocess_text)\n",
    "\n",
    "# Combine subject and body for model input\n",
    "df_combined['text'] = df_combined['subject_clean'] + ' ' + df_combined['body_clean']\n",
    "df_combined['text'] = df_combined['text'].str.strip()\n",
    "\n",
    "# Calculate text statistics\n",
    "df_combined['text_length'] = df_combined['text'].str.len()\n",
    "df_combined['word_count'] = df_combined['text'].str.split().str.len()\n",
    "\n",
    "print('Text preprocessing complete!')\n",
    "print(f'\\nText length statistics:')\n",
    "print(df_combined['text_length'].describe())\n",
    "print(f'\\nWord count statistics:')\n",
    "print(df_combined['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for very short or very long emails\n",
    "print('Email length distribution:')\n",
    "print('='*70)\n",
    "print(f'Very short (< 10 chars): {(df_combined[\"text_length\"] < 10).sum():,}')\n",
    "print(f'Short (10-100 chars): {((df_combined[\"text_length\"] >= 10) & (df_combined[\"text_length\"] < 100)).sum():,}')\n",
    "print(f'Medium (100-1000 chars): {((df_combined[\"text_length\"] >= 100) & (df_combined[\"text_length\"] < 1000)).sum():,}')\n",
    "print(f'Long (1000-5000 chars): {((df_combined[\"text_length\"] >= 1000) & (df_combined[\"text_length\"] < 5000)).sum():,}')\n",
    "print(f'Very long (> 5000 chars): {(df_combined[\"text_length\"] >= 5000).sum():,}')\n",
    "\n",
    "# Sample some texts\n",
    "print('\\nSample emails:')\n",
    "print('='*70)\n",
    "print('\\nLegitimate email sample:')\n",
    "print(df_combined[df_combined['label']==0]['text'].iloc[0][:200] + '...')\n",
    "print('\\nPhishing email sample:')\n",
    "print(df_combined[df_combined['label']==1]['text'].iloc[0][:200] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FINAL DATASET SUMMARY')\n",
    "print('='*70)\n",
    "print(f'Total emails: {len(df_combined):,}')\n",
    "print(f'Shape: {df_combined.shape}')\n",
    "print(f'\\nColumns: {list(df_combined.columns)}')\n",
    "\n",
    "print('\\nLabel distribution:')\n",
    "label_dist = df_combined['label'].value_counts().sort_index()\n",
    "for label, count in label_dist.items():\n",
    "    print(f'  Label {label}: {count:>7,} ({count/len(df_combined)*100:>5.2f}%)')\n",
    "\n",
    "print('\\nClass balance:')\n",
    "balance_ratio = label_dist.min() / label_dist.max()\n",
    "print(f'  Ratio (minority/majority): {balance_ratio:.3f}')\n",
    "if balance_ratio < 0.5:\n",
    "    print('  ⚠ Dataset is imbalanced - consider using class weights or resampling')\n",
    "else:\n",
    "    print('  ✓ Dataset is reasonably balanced')\n",
    "\n",
    "print('\\nSource distribution:')\n",
    "for source, count in df_combined['source'].value_counts().items():\n",
    "    print(f'  {source:15s}: {count:>7,} ({count/len(df_combined)*100:>5.2f}%)')\n",
    "\n",
    "print('\\nText statistics by label:')\n",
    "print(df_combined.groupby('label')[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataset with text and metadata columns\n",
    "df_final = df_combined[['sender', 'receiver', 'date', 'text', 'urls', 'label', 'source']].copy()\n",
    "\n",
    "# Stratified split: 70% train, 15% validation, 15% test\n",
    "print('Creating train/validation/test split...')\n",
    "print('='*70)\n",
    "\n",
    "# First split: 70% train, 30% temp (for val + test)\n",
    "df_train, df_temp = train_test_split(\n",
    "    df_final,\n",
    "    test_size=0.30,\n",
    "    stratify=df_final['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 50% of temp for validation, 50% for test (15% each of total)\n",
    "df_val, df_test = train_test_split(\n",
    "    df_temp,\n",
    "    test_size=0.50,\n",
    "    stratify=df_temp['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f'Train set: {len(df_train):>7,} ({len(df_train)/len(df_final)*100:>5.2f}%)')\n",
    "print(f'Val set:   {len(df_val):>7,} ({len(df_val)/len(df_final)*100:>5.2f}%)')\n",
    "print(f'Test set:  {len(df_test):>7,} ({len(df_test)/len(df_final)*100:>5.2f}%)')\n",
    "print(f'Total:     {len(df_final):>7,}')\n",
    "\n",
    "print('\\nLabel distribution in splits:')\n",
    "print('\\nTrain:')\n",
    "print(df_train['label'].value_counts().sort_index())\n",
    "print('\\nValidation:')\n",
    "print(df_val['label'].value_counts().sort_index())\n",
    "print('\\nTest:')\n",
    "print(df_test['label'].value_counts().sort_index())\n",
    "\n",
    "print('\\nColumns in final datasets:')\n",
    "print(list(df_final.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving processed datasets...')\n",
    "print('='*70)\n",
    "\n",
    "# Save to CSV\n",
    "train_path = PROCESSED_PATH / 'train.csv'\n",
    "val_path = PROCESSED_PATH / 'val.csv'\n",
    "test_path = PROCESSED_PATH / 'test.csv'\n",
    "full_path = PROCESSED_PATH / 'full_processed.csv'\n",
    "\n",
    "df_train.to_csv(train_path, index=False)\n",
    "df_val.to_csv(val_path, index=False)\n",
    "df_test.to_csv(test_path, index=False)\n",
    "df_final.to_csv(full_path, index=False)\n",
    "\n",
    "print(f'✓ Train set saved to: {train_path}')\n",
    "print(f'  Size: {train_path.stat().st_size / (1024*1024):.2f} MB')\n",
    "print(f'✓ Validation set saved to: {val_path}')\n",
    "print(f'  Size: {val_path.stat().st_size / (1024*1024):.2f} MB')\n",
    "print(f'✓ Test set saved to: {test_path}')\n",
    "print(f'  Size: {test_path.stat().st_size / (1024*1024):.2f} MB')\n",
    "print(f'✓ Full dataset saved to: {full_path}')\n",
    "print(f'  Size: {full_path.stat().st_size / (1024*1024):.2f} MB')\n",
    "\n",
    "print('\\n✓ All datasets saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary report\n",
    "report = f\"\"\"\n",
    "{'='*70}\n",
    "DATA PREPROCESSING REPORT\n",
    "{'='*70}\n",
    "\n",
    "INPUT DATA:\n",
    "  - Total raw emails: {sum(len(df) for df in datasets.values()):,}\n",
    "  - Datasets: {len(datasets)}\n",
    "\n",
    "CLEANING:\n",
    "  - Garbage labels removed: {total_removed:,}\n",
    "  - Clean emails retained: {total_clean:,}\n",
    "  - Retention rate: {total_clean/(total_clean+total_removed)*100:.2f}%\n",
    "\n",
    "FINAL DATASET:\n",
    "  - Total emails: {len(df_final):,}\n",
    "  - Legitimate (0): {(df_final['label']==0).sum():,}\n",
    "  - Phishing (1): {(df_final['label']==1).sum():,}\n",
    "  - Class balance ratio: {balance_ratio:.3f}\n",
    "\n",
    "SPLITS:\n",
    "  - Train: {len(df_train):,} ({len(df_train)/len(df_final)*100:.1f}%)\n",
    "  - Validation: {len(df_val):,} ({len(df_val)/len(df_final)*100:.1f}%)\n",
    "  - Test: {len(df_test):,} ({len(df_test)/len(df_final)*100:.1f}%)\n",
    "\n",
    "TEXT STATISTICS:\n",
    "  - Average text length: {df_final['text'].str.len().mean():.0f} characters\n",
    "  - Average word count: {df_final['text'].str.split().str.len().mean():.0f} words\n",
    "  - Max text length: {df_final['text'].str.len().max():,} characters\n",
    "\n",
    "OUTPUT FILES:\n",
    "  - {train_path}\n",
    "  - {val_path}\n",
    "  - {test_path}\n",
    "  - {full_path}\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_path = PROCESSED_PATH / 'preprocessing_report.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f'\\n✓ Report saved to: {report_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
