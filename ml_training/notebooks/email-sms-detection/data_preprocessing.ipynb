{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Phishing Dataset - Data Preprocessing\n",
    "\n",
    "This notebook performs data cleaning and preprocessing for the BERT-LSTM phishing detection model.\n",
    "\n",
    "## Steps:\n",
    "1. Load all datasets\n",
    "2. Filter out garbage labels (keep only 0 and 1)\n",
    "3. Standardize schema with metadata fields (sender, receiver, date, urls)\n",
    "4. Combine all clean datasets\n",
    "5. Text preprocessing and cleaning\n",
    "6. Train/validation/test split\n",
    "7. Save processed data\n",
    "\n",
    "## Final Schema:\n",
    "- **sender** - Email sender (null for datasets without metadata)\n",
    "- **receiver** - Email receiver (null for datasets without metadata)\n",
    "- **date** - Email date (null for datasets without metadata)\n",
    "- **text** - Combined subject + body\n",
    "- **urls** - URL information (null for datasets without metadata)\n",
    "- **label** - 0 (legitimate) or 1 (phishing)\n",
    "- **source** - Original dataset name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: ..\\..\\..\\data\\unprocessed\\email-detection\\Seven Emails phishing dataset\n",
      "Processed data path: ..\\..\\..\\data\\processed\\email-detection\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_PATH = Path('../../../data/unprocessed/email-detection/Seven Emails phishing dataset')\n",
    "PROCESSED_PATH = Path('../../../data/processed/email-detection')\n",
    "\n",
    "# Create processed data directory\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Raw data path: {DATA_PATH}')\n",
    "print(f'Processed data path: {PROCESSED_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "======================================================================\n",
      "✓ Assassin.csv         -   5,809 rows - loaded_successfully\n",
      "✓ CEAS-08.csv          -  39,154 rows - loaded_successfully\n",
      "✓ Enron.csv            -  29,767 rows - loaded_successfully\n",
      "✓ Ling.csv             -   2,859 rows - loaded_successfully\n",
      "✓ TREC-05.csv          -  59,015 rows - loaded_with_skipped_lines\n",
      "✓ TREC-06.csv          -  16,439 rows - loaded_with_skipped_lines\n",
      "✓ TREC-07.csv          -  53,757 rows - loaded_successfully\n",
      "\n",
      "Total datasets loaded: 7\n",
      "Total raw emails: 206,800\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(file_path, file_name):\n",
    "    \"\"\"Load dataset with appropriate error handling\"\"\"\n",
    "    try:\n",
    "        if file_name in ['TREC-05.csv', 'TREC-06.csv']:\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                on_bad_lines='skip',\n",
    "                engine='python',\n",
    "                encoding='utf-8',\n",
    "                quoting=1\n",
    "            )\n",
    "            return df, 'loaded_with_skipped_lines'\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "            return df, 'loaded_successfully'\n",
    "    except Exception as e:\n",
    "        return None, f'error: {str(e)}'\n",
    "\n",
    "csv_files = ['Assassin.csv', 'CEAS-08.csv', 'Enron.csv', 'Ling.csv', 'TREC-05.csv', 'TREC-06.csv', 'TREC-07.csv']\n",
    "datasets = {}\n",
    "\n",
    "print('Loading datasets...')\n",
    "print('='*70)\n",
    "for file in csv_files:\n",
    "    df, status = load_dataset(DATA_PATH / file, file)\n",
    "    if df is not None:\n",
    "        name = file.replace('.csv', '')\n",
    "        datasets[name] = df\n",
    "        print(f'✓ {file:20s} - {len(df):>7,} rows - {status}')\n",
    "    else:\n",
    "        print(f'✗ {file:20s} - {status}')\n",
    "\n",
    "print(f'\\nTotal datasets loaded: {len(datasets)}')\n",
    "print(f'Total raw emails: {sum(len(df) for df in datasets.values()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter and Clean Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning labels (keeping only 0 and 1)...\n",
      "======================================================================\n",
      "Assassin        - Original:  5,809 | Clean:  5,809 | Removed:     0 ( 0.00%)\n",
      "CEAS-08         - Original: 39,154 | Clean: 39,154 | Removed:     0 ( 0.00%)\n",
      "Enron           - Original: 29,767 | Clean: 29,767 | Removed:     0 ( 0.00%)\n",
      "Ling            - Original:  2,859 | Clean:  2,859 | Removed:     0 ( 0.00%)\n",
      "TREC-05         - Original: 59,015 | Clean: 55,210 | Removed: 3,805 ( 6.45%)\n",
      "TREC-06         - Original: 16,439 | Clean: 16,382 | Removed:    57 ( 0.35%)\n",
      "TREC-07         - Original: 53,757 | Clean: 53,757 | Removed:     0 ( 0.00%)\n",
      "======================================================================\n",
      "Total clean emails: 202,938\n",
      "Total removed: 3,862\n",
      "Retention rate: 98.13%\n"
     ]
    }
   ],
   "source": [
    "def clean_labels(df, dataset_name):\n",
    "    \"\"\"Filter dataset to keep only valid binary labels (0 and 1)\"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Convert labels to numeric, coerce errors to NaN\n",
    "    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
    "    \n",
    "    # Filter to keep only 0 and 1\n",
    "    df_clean = df[df['label'].isin([0, 1])].copy()\n",
    "    \n",
    "    # Convert to integer\n",
    "    df_clean['label'] = df_clean['label'].astype(int)\n",
    "    \n",
    "    removed = original_count - len(df_clean)\n",
    "    \n",
    "    print(f'{dataset_name:15s} - Original: {original_count:>6,} | Clean: {len(df_clean):>6,} | Removed: {removed:>5,} ({removed/original_count*100:>5.2f}%)')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "print('Cleaning labels (keeping only 0 and 1)...')\n",
    "print('='*70)\n",
    "\n",
    "cleaned_datasets = {}\n",
    "for name, df in datasets.items():\n",
    "    cleaned_datasets[name] = clean_labels(df, name)\n",
    "\n",
    "total_clean = sum(len(df) for df in cleaned_datasets.values())\n",
    "total_removed = sum(len(datasets[name]) - len(cleaned_datasets[name]) for name in datasets.keys())\n",
    "\n",
    "print('='*70)\n",
    "print(f'Total clean emails: {total_clean:,}')\n",
    "print(f'Total removed: {total_removed:,}')\n",
    "print(f'Retention rate: {total_clean/(total_clean+total_removed)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Standardize Schema and Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing schemas...\n",
      "======================================================================\n",
      "✓ Assassin        -  5,809 emails - Full (with metadata)\n",
      "✓ CEAS-08         - 39,154 emails - Full (with metadata)\n",
      "✓ Enron           - 29,767 emails - Minimal (text only)\n",
      "✓ Ling            -  2,859 emails - Minimal (text only)\n",
      "✓ TREC-05         - 55,210 emails - Full (with metadata)\n",
      "✓ TREC-06         - 16,382 emails - Full (with metadata)\n",
      "✓ TREC-07         - 53,757 emails - Full (with metadata)\n",
      "\n",
      "Combining datasets...\n",
      "\n",
      "Combined dataset shape: (202938, 8)\n",
      "Columns: ['sender', 'receiver', 'date', 'urls', 'subject', 'body', 'label', 'source']\n",
      "\n",
      "Metadata availability:\n",
      "  Emails with sender: 170,030 (83.8%)\n",
      "  Emails with receiver: 167,034 (82.3%)\n",
      "  Emails with date: 168,491 (83.0%)\n",
      "  Emails with urls: 170,312 (83.9%)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0    108624\n",
      "1     94314\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Source distribution:\n",
      "source\n",
      "TREC-05     55210\n",
      "TREC-07     53757\n",
      "CEAS-08     39154\n",
      "Enron       29767\n",
      "TREC-06     16382\n",
      "Assassin     5809\n",
      "Ling         2859\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def standardize_schema(df, dataset_name):\n",
    "    \"\"\"Standardize schema to include metadata fields: sender, receiver, date, subject, body, urls, label, source\"\"\"\n",
    "    \n",
    "    # Ensure subject and body exist\n",
    "    if 'subject' not in df.columns or 'body' not in df.columns:\n",
    "        print(f'Warning: {dataset_name} missing subject or body columns')\n",
    "        return None\n",
    "    \n",
    "    # Start with required columns\n",
    "    df_std = pd.DataFrame()\n",
    "    \n",
    "    # Add metadata columns (if they exist, otherwise add as None)\n",
    "    df_std['sender'] = df['sender'] if 'sender' in df.columns else None\n",
    "    df_std['receiver'] = df['receiver'] if 'receiver' in df.columns else None\n",
    "    df_std['date'] = df['date'] if 'date' in df.columns else None\n",
    "    df_std['urls'] = df['urls'] if 'urls' in df.columns else None\n",
    "    \n",
    "    # Add text columns\n",
    "    df_std['subject'] = df['subject']\n",
    "    df_std['body'] = df['body']\n",
    "    \n",
    "    # Add label and source\n",
    "    df_std['label'] = df['label']\n",
    "    df_std['source'] = dataset_name\n",
    "    \n",
    "    return df_std\n",
    "\n",
    "print('Standardizing schemas...')\n",
    "print('='*70)\n",
    "\n",
    "standardized_datasets = []\n",
    "for name, df in cleaned_datasets.items():\n",
    "    df_std = standardize_schema(df, name)\n",
    "    if df_std is not None:\n",
    "        standardized_datasets.append(df_std)\n",
    "        has_metadata = 'sender' in df.columns and 'urls' in df.columns\n",
    "        schema_type = 'Full (with metadata)' if has_metadata else 'Minimal (text only)'\n",
    "        print(f'✓ {name:15s} - {len(df_std):>6,} emails - {schema_type}')\n",
    "\n",
    "# Combine all datasets\n",
    "print('\\nCombining datasets...')\n",
    "df_combined = pd.concat(standardized_datasets, ignore_index=True)\n",
    "\n",
    "print(f'\\nCombined dataset shape: {df_combined.shape}')\n",
    "print(f'Columns: {list(df_combined.columns)}')\n",
    "\n",
    "# Check metadata availability\n",
    "print(f'\\nMetadata availability:')\n",
    "sender_count = df_combined['sender'].notna().sum()\n",
    "receiver_count = df_combined['receiver'].notna().sum()\n",
    "date_count = df_combined['date'].notna().sum()\n",
    "urls_count = df_combined['urls'].notna().sum()\n",
    "total_count = len(df_combined)\n",
    "\n",
    "sender_pct = sender_count / total_count * 100\n",
    "receiver_pct = receiver_count / total_count * 100\n",
    "date_pct = date_count / total_count * 100\n",
    "urls_pct = urls_count / total_count * 100\n",
    "\n",
    "print(f'  Emails with sender: {sender_count:,} ({sender_pct:.1f}%)')\n",
    "print(f'  Emails with receiver: {receiver_count:,} ({receiver_pct:.1f}%)')\n",
    "print(f'  Emails with date: {date_count:,} ({date_pct:.1f}%)')\n",
    "print(f'  Emails with urls: {urls_count:,} ({urls_pct:.1f}%)')\n",
    "\n",
    "print(f'\\nLabel distribution:')\n",
    "print(df_combined['label'].value_counts().sort_index())\n",
    "print(f'\\nSource distribution:')\n",
    "print(df_combined['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before cleaning:\n",
      "======================================================================\n",
      "sender      32908\n",
      "receiver    35904\n",
      "date        34447\n",
      "urls        32626\n",
      "subject      2405\n",
      "body            3\n",
      "label           0\n",
      "source          0\n",
      "dtype: int64\n",
      "\n",
      "Rows with missing subject: 2405\n",
      "Rows with missing body: 3\n",
      "\n",
      "Rows dropped (empty subject AND body): 0\n",
      "Remaining rows: 202,938\n"
     ]
    }
   ],
   "source": [
    "print('Missing values before cleaning:')\n",
    "print('='*70)\n",
    "print(df_combined.isnull().sum())\n",
    "print(f'\\nRows with missing subject: {df_combined[\"subject\"].isnull().sum()}')\n",
    "print(f'Rows with missing body: {df_combined[\"body\"].isnull().sum()}')\n",
    "\n",
    "# Fill missing values with empty string\n",
    "df_combined['subject'] = df_combined['subject'].fillna('')\n",
    "df_combined['body'] = df_combined['body'].fillna('')\n",
    "\n",
    "# Drop rows where both subject and body are empty\n",
    "before_drop = len(df_combined)\n",
    "df_combined = df_combined[\n",
    "    (df_combined['subject'].astype(str).str.strip() != '') | \n",
    "    (df_combined['body'].astype(str).str.strip() != '')\n",
    "].copy()\n",
    "after_drop = len(df_combined)\n",
    "\n",
    "print(f'\\nRows dropped (empty subject AND body): {before_drop - after_drop}')\n",
    "print(f'Remaining rows: {after_drop:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text...\n",
      "======================================================================\n",
      "Text preprocessing complete!\n",
      "\n",
      "Text length statistics:\n",
      "count    202938.000000\n",
      "mean       1613.441992\n",
      "std        3981.321365\n",
      "min           3.000000\n",
      "25%         360.000000\n",
      "50%         774.000000\n",
      "75%        1693.000000\n",
      "max      641703.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Word count statistics:\n",
      "count    202938.000000\n",
      "mean        258.818925\n",
      "std         611.793552\n",
      "min           1.000000\n",
      "25%          58.000000\n",
      "50%         128.000000\n",
      "75%         281.000000\n",
      "max       63380.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove very long sequences of repeated characters (likely noise)\n",
    "    text = re.sub(r'(.)\\1{10,}', r'\\1\\1\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "print('Preprocessing text...')\n",
    "print('='*70)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_combined['subject_clean'] = df_combined['subject'].apply(preprocess_text)\n",
    "df_combined['body_clean'] = df_combined['body'].apply(preprocess_text)\n",
    "\n",
    "# Combine subject and body for model input\n",
    "df_combined['text'] = df_combined['subject_clean'] + ' ' + df_combined['body_clean']\n",
    "df_combined['text'] = df_combined['text'].str.strip()\n",
    "\n",
    "# Calculate text statistics\n",
    "df_combined['text_length'] = df_combined['text'].str.len()\n",
    "df_combined['word_count'] = df_combined['text'].str.split().str.len()\n",
    "\n",
    "print('Text preprocessing complete!')\n",
    "print(f'\\nText length statistics:')\n",
    "print(df_combined['text_length'].describe())\n",
    "print(f'\\nWord count statistics:')\n",
    "print(df_combined['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email length distribution:\n",
      "======================================================================\n",
      "Very short (< 10 chars): 6\n",
      "Short (10-100 chars): 5,940\n",
      "Medium (100-1000 chars): 114,662\n",
      "Long (1000-5000 chars): 72,045\n",
      "Very long (> 5000 chars): 10,285\n",
      "\n",
      "Sample emails:\n",
      "======================================================================\n",
      "\n",
      "Legitimate email sample:\n",
      "Re: New Sequences Window Date: Wed, 21 Aug 2002 10:54:46 -0500 From: Chris Garrigues Message-ID: <1029945287.4797.TMDA@deepeddy.vircio.com> | I can't reproduce this error. For me it is very repeatable...\n",
      "\n",
      "Phishing email sample:\n",
      "Life Insurance - Why Pay More? * * * --- * * * <= /TR> Save up to 70% on Life Insurance. Why Spend More Than You Have To? Life Quote Savings __** _**__ **Ensurin= g your family's financial security is...\n"
     ]
    }
   ],
   "source": [
    "# Check for very short or very long emails\n",
    "print('Email length distribution:')\n",
    "print('='*70)\n",
    "print(f'Very short (< 10 chars): {(df_combined[\"text_length\"] < 10).sum():,}')\n",
    "print(f'Short (10-100 chars): {((df_combined[\"text_length\"] >= 10) & (df_combined[\"text_length\"] < 100)).sum():,}')\n",
    "print(f'Medium (100-1000 chars): {((df_combined[\"text_length\"] >= 100) & (df_combined[\"text_length\"] < 1000)).sum():,}')\n",
    "print(f'Long (1000-5000 chars): {((df_combined[\"text_length\"] >= 1000) & (df_combined[\"text_length\"] < 5000)).sum():,}')\n",
    "print(f'Very long (> 5000 chars): {(df_combined[\"text_length\"] >= 5000).sum():,}')\n",
    "\n",
    "# Sample some texts\n",
    "print('\\nSample emails:')\n",
    "print('='*70)\n",
    "print('\\nLegitimate email sample:')\n",
    "print(df_combined[df_combined['label']==0]['text'].iloc[0][:200] + '...')\n",
    "print('\\nPhishing email sample:')\n",
    "print(df_combined[df_combined['label']==1]['text'].iloc[0][:200] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL DATASET SUMMARY\n",
      "======================================================================\n",
      "Total emails: 202,938\n",
      "Shape: (202938, 13)\n",
      "\n",
      "Columns: ['sender', 'receiver', 'date', 'urls', 'subject', 'body', 'label', 'source', 'subject_clean', 'body_clean', 'text', 'text_length', 'word_count']\n",
      "\n",
      "Label distribution:\n",
      "  Label 0: 108,624 (53.53%)\n",
      "  Label 1:  94,314 (46.47%)\n",
      "\n",
      "Class balance:\n",
      "  Ratio (minority/majority): 0.868\n",
      "  ✓ Dataset is reasonably balanced\n",
      "\n",
      "Source distribution:\n",
      "  TREC-05        :  55,210 (27.21%)\n",
      "  TREC-07        :  53,757 (26.49%)\n",
      "  CEAS-08        :  39,154 (19.29%)\n",
      "  Enron          :  29,767 (14.67%)\n",
      "  TREC-06        :  16,382 ( 8.07%)\n",
      "  Assassin       :   5,809 ( 2.86%)\n",
      "  Ling           :   2,859 ( 1.41%)\n",
      "\n",
      "Text statistics by label:\n",
      "      text_length                                                        \\\n",
      "            count         mean          std  min    25%     50%     75%   \n",
      "label                                                                     \n",
      "0        108624.0  2016.715109  4995.416027  9.0  516.0  1025.0  1997.0   \n",
      "1         94314.0  1148.981371  2227.906391  3.0  260.0   562.0  1216.0   \n",
      "\n",
      "                word_count                                                   \\\n",
      "            max      count        mean         std  min   25%    50%    75%   \n",
      "label                                                                         \n",
      "0      641703.0   108624.0  327.497395  775.179038  2.0  84.0  168.0  333.0   \n",
      "1      230514.0    94314.0  179.720063  318.765647  1.0  39.0   93.0  202.0   \n",
      "\n",
      "                \n",
      "           max  \n",
      "label           \n",
      "0      63380.0  \n",
      "1      24683.0  \n"
     ]
    }
   ],
   "source": [
    "print('FINAL DATASET SUMMARY')\n",
    "print('='*70)\n",
    "print(f'Total emails: {len(df_combined):,}')\n",
    "print(f'Shape: {df_combined.shape}')\n",
    "print(f'\\nColumns: {list(df_combined.columns)}')\n",
    "\n",
    "print('\\nLabel distribution:')\n",
    "label_dist = df_combined['label'].value_counts().sort_index()\n",
    "for label, count in label_dist.items():\n",
    "    print(f'  Label {label}: {count:>7,} ({count/len(df_combined)*100:>5.2f}%)')\n",
    "\n",
    "print('\\nClass balance:')\n",
    "balance_ratio = label_dist.min() / label_dist.max()\n",
    "print(f'  Ratio (minority/majority): {balance_ratio:.3f}')\n",
    "if balance_ratio < 0.5:\n",
    "    print('  ⚠ Dataset is imbalanced - consider using class weights or resampling')\n",
    "else:\n",
    "    print('  ✓ Dataset is reasonably balanced')\n",
    "\n",
    "print('\\nSource distribution:')\n",
    "for source, count in df_combined['source'].value_counts().items():\n",
    "    print(f'  {source:15s}: {count:>7,} ({count/len(df_combined)*100:>5.2f}%)')\n",
    "\n",
    "print('\\nText statistics by label:')\n",
    "print(df_combined.groupby('label')[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/validation/test split...\n",
      "======================================================================\n",
      "Train set: 142,056 (70.00%)\n",
      "Val set:    30,441 (15.00%)\n",
      "Test set:   30,441 (15.00%)\n",
      "Total:     202,938\n",
      "\n",
      "Label distribution in splits:\n",
      "\n",
      "Train:\n",
      "label\n",
      "0    76036\n",
      "1    66020\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation:\n",
      "label\n",
      "0    16294\n",
      "1    14147\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test:\n",
      "label\n",
      "0    16294\n",
      "1    14147\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Columns in final datasets:\n",
      "['sender', 'receiver', 'date', 'text', 'urls', 'label', 'source']\n"
     ]
    }
   ],
   "source": [
    "# Prepare final dataset with text and metadata columns\n",
    "df_final = df_combined[['sender', 'receiver', 'date', 'text', 'urls', 'label', 'source']].copy()\n",
    "\n",
    "# Stratified split: 70% train, 15% validation, 15% test\n",
    "print('Creating train/validation/test split...')\n",
    "print('='*70)\n",
    "\n",
    "# First split: 70% train, 30% temp (for val + test)\n",
    "df_train, df_temp = train_test_split(\n",
    "    df_final,\n",
    "    test_size=0.30,\n",
    "    stratify=df_final['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 50% of temp for validation, 50% for test (15% each of total)\n",
    "df_val, df_test = train_test_split(\n",
    "    df_temp,\n",
    "    test_size=0.50,\n",
    "    stratify=df_temp['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f'Train set: {len(df_train):>7,} ({len(df_train)/len(df_final)*100:>5.2f}%)')\n",
    "print(f'Val set:   {len(df_val):>7,} ({len(df_val)/len(df_final)*100:>5.2f}%)')\n",
    "print(f'Test set:  {len(df_test):>7,} ({len(df_test)/len(df_final)*100:>5.2f}%)')\n",
    "print(f'Total:     {len(df_final):>7,}')\n",
    "\n",
    "print('\\nLabel distribution in splits:')\n",
    "print('\\nTrain:')\n",
    "print(df_train['label'].value_counts().sort_index())\n",
    "print('\\nValidation:')\n",
    "print(df_val['label'].value_counts().sort_index())\n",
    "print('\\nTest:')\n",
    "print(df_test['label'].value_counts().sort_index())\n",
    "\n",
    "print('\\nColumns in final datasets:')\n",
    "print(list(df_final.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed datasets...\n",
      "======================================================================\n",
      "✓ Train set saved to: ..\\..\\..\\data\\processed\\email-detection\\train.csv\n",
      "  Size: 238.36 MB\n",
      "✓ Validation set saved to: ..\\..\\..\\data\\processed\\email-detection\\val.csv\n",
      "  Size: 51.71 MB\n",
      "✓ Test set saved to: ..\\..\\..\\data\\processed\\email-detection\\test.csv\n",
      "  Size: 51.80 MB\n",
      "✓ Full dataset saved to: ..\\..\\..\\data\\processed\\email-detection\\full_processed.csv\n",
      "  Size: 341.87 MB\n",
      "\n",
      "✓ All datasets saved successfully!\n"
     ]
    }
   ],
   "source": [
    "print('Saving processed datasets...')\n",
    "print('='*70)\n",
    "\n",
    "# Save to CSV\n",
    "train_path = PROCESSED_PATH / 'train.csv'\n",
    "val_path = PROCESSED_PATH / 'val.csv'\n",
    "test_path = PROCESSED_PATH / 'test.csv'\n",
    "full_path = PROCESSED_PATH / 'full_processed.csv'\n",
    "\n",
    "df_train.to_csv(train_path, index=False)\n",
    "df_val.to_csv(val_path, index=False)\n",
    "df_test.to_csv(test_path, index=False)\n",
    "df_final.to_csv(full_path, index=False)\n",
    "\n",
    "print(f'✓ Train set saved to: {train_path}')\n",
    "print(f'  Size: {train_path.stat().st_size / (1024*1024):.2f} MB')\n",
    "print(f'✓ Validation set saved to: {val_path}')\n",
    "print(f'  Size: {val_path.stat().st_size / (1024*1024):.2f} MB')\n",
    "print(f'✓ Test set saved to: {test_path}')\n",
    "print(f'  Size: {test_path.stat().st_size / (1024*1024):.2f} MB')\n",
    "print(f'✓ Full dataset saved to: {full_path}')\n",
    "print(f'  Size: {full_path.stat().st_size / (1024*1024):.2f} MB')\n",
    "\n",
    "print('\\n✓ All datasets saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA PREPROCESSING REPORT\n",
      "======================================================================\n",
      "\n",
      "INPUT DATA:\n",
      "  - Total raw emails: 206,800\n",
      "  - Datasets: 7\n",
      "\n",
      "CLEANING:\n",
      "  - Garbage labels removed: 3,862\n",
      "  - Clean emails retained: 202,938\n",
      "  - Retention rate: 98.13%\n",
      "\n",
      "FINAL DATASET:\n",
      "  - Total emails: 202,938\n",
      "  - Legitimate (0): 108,624\n",
      "  - Phishing (1): 94,314\n",
      "  - Class balance ratio: 0.868\n",
      "\n",
      "SPLITS:\n",
      "  - Train: 142,056 (70.0%)\n",
      "  - Validation: 30,441 (15.0%)\n",
      "  - Test: 30,441 (15.0%)\n",
      "\n",
      "TEXT STATISTICS:\n",
      "  - Average text length: 1613 characters\n",
      "  - Average word count: 259 words\n",
      "  - Max text length: 641,703 characters\n",
      "\n",
      "OUTPUT FILES:\n",
      "  - ..\\..\\..\\data\\processed\\email-detection\\train.csv\n",
      "  - ..\\..\\..\\data\\processed\\email-detection\\val.csv\n",
      "  - ..\\..\\..\\data\\processed\\email-detection\\test.csv\n",
      "  - ..\\..\\..\\data\\processed\\email-detection\\full_processed.csv\n",
      "\n",
      "READY FOR BERT-LSTM MODEL TRAINING!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "✓ Report saved to: ..\\..\\..\\data\\processed\\email-detection\\preprocessing_report.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a summary report\n",
    "report = f\"\"\"\n",
    "{'='*70}\n",
    "DATA PREPROCESSING REPORT\n",
    "{'='*70}\n",
    "\n",
    "INPUT DATA:\n",
    "  - Total raw emails: {sum(len(df) for df in datasets.values()):,}\n",
    "  - Datasets: {len(datasets)}\n",
    "\n",
    "CLEANING:\n",
    "  - Garbage labels removed: {total_removed:,}\n",
    "  - Clean emails retained: {total_clean:,}\n",
    "  - Retention rate: {total_clean/(total_clean+total_removed)*100:.2f}%\n",
    "\n",
    "FINAL DATASET:\n",
    "  - Total emails: {len(df_final):,}\n",
    "  - Legitimate (0): {(df_final['label']==0).sum():,}\n",
    "  - Phishing (1): {(df_final['label']==1).sum():,}\n",
    "  - Class balance ratio: {balance_ratio:.3f}\n",
    "\n",
    "SPLITS:\n",
    "  - Train: {len(df_train):,} ({len(df_train)/len(df_final)*100:.1f}%)\n",
    "  - Validation: {len(df_val):,} ({len(df_val)/len(df_final)*100:.1f}%)\n",
    "  - Test: {len(df_test):,} ({len(df_test)/len(df_final)*100:.1f}%)\n",
    "\n",
    "TEXT STATISTICS:\n",
    "  - Average text length: {df_final['text'].str.len().mean():.0f} characters\n",
    "  - Average word count: {df_final['text'].str.split().str.len().mean():.0f} words\n",
    "  - Max text length: {df_final['text'].str.len().max():,} characters\n",
    "\n",
    "OUTPUT FILES:\n",
    "  - {train_path}\n",
    "  - {val_path}\n",
    "  - {test_path}\n",
    "  - {full_path}\n",
    "\n",
    "READY FOR BERT-LSTM MODEL TRAINING!\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_path = PROCESSED_PATH / 'preprocessing_report.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f'\\n✓ Report saved to: {report_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What was done:\n",
    "1. ✅ Loaded 7 email datasets\n",
    "2. ✅ Filtered out garbage labels (kept only 0 and 1)\n",
    "3. ✅ Standardized schema with metadata (sender, receiver, date, text, urls, label, source)\n",
    "4. ✅ Combined all datasets into one\n",
    "5. ✅ Handled missing values\n",
    "6. ✅ Preprocessed text (cleaned, combined subject + body into 'text' field)\n",
    "7. ✅ Created stratified train/val/test splits (70/15/15)\n",
    "8. ✅ Saved processed datasets\n",
    "\n",
    "### Schema Details:\n",
    "- **Datasets with full metadata** (sender, receiver, date, urls): Assassin, CEAS-08, TREC-05, TREC-06, TREC-07\n",
    "- **Datasets with minimal schema** (text only): Enron, Ling\n",
    "- All datasets combined with null values for missing metadata fields\n",
    "\n",
    "### Next Steps:\n",
    "1. Build BERT-LSTM model architecture\n",
    "2. Implement BERT tokenization\n",
    "3. Create data loaders (PyTorch/TensorFlow)\n",
    "4. Train the model\n",
    "5. Evaluate performance\n",
    "\n",
    "### Files Created:\n",
    "- `data/processed/email-detection/train.csv` - Training set (sender, receiver, date, text, urls, label, source)\n",
    "- `data/processed/email-detection/val.csv` - Validation set\n",
    "- `data/processed/email-detection/test.csv` - Test set\n",
    "- `data/processed/email-detection/full_processed.csv` - Complete dataset\n",
    "- `data/processed/email-detection/preprocessing_report.txt` - Summary report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
