{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 1: LOAD THE DATASET\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = 'E:/.cache/huggingface'\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Loading PhreshPhish dataset...\")\n",
    "dataset = load_dataset(\"phreshphish/phreshphish\", cache_dir='E:/.cache/huggingface')\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\")\n",
    "print(f\"Available splits: {dataset.keys()}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: DATASET SIZE INFO\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET SIZE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train set size: {len(dataset['train']):,} samples\")\n",
    "print(f\"Test set size: {len(dataset['test']):,} samples\")\n",
    "print(f\"Total samples: {len(dataset['train']) + len(dataset['test']):,}\")\n",
    "\n",
    "print(\"\\nDataset is HUGE (>100GB with HTML). Using 5000 samples for EDA...\")\n",
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "train_sample = dataset['train'].select(range(min(SAMPLE_SIZE, len(dataset['train']))))\n",
    "test_sample = dataset['test'].select(range(min(SAMPLE_SIZE, len(dataset['test']))))\n",
    "\n",
    "# Convert samples to pandas\n",
    "train_df = pd.DataFrame(train_sample)\n",
    "test_df = pd.DataFrame(test_sample)\n",
    "\n",
    "print(f\"Working with {len(train_df):,} train samples and {len(test_df):,} test samples\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: BASIC DATASET STRUCTURE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET STRUCTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(train_df.dtypes)\n",
    "\n",
    "print(\"\\nDataset shape (rows, columns):\")\n",
    "print(f\"Train sample: {train_df.shape}\")\n",
    "print(f\"Test sample: {test_df.shape}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: LOOK AT SAMPLE ROWS (WITHOUT HTML)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE DATA (First 3 rows, excluding HTML)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show all columns except HTML (which is too large)\n",
    "cols_to_show = [col for col in train_df.columns if col != 'html']\n",
    "print(train_df[cols_to_show].head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE DATA (Random 3 rows, excluding HTML)\")\n",
    "print(\"=\"*60)\n",
    "print(train_df[cols_to_show].sample(3, random_state=42))\n",
    "\n",
    "# ============================================\n",
    "# STEP 5: HTML COLUMN ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HTML COLUMN SIZE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'html' in train_df.columns:\n",
    "    html_lengths = train_df['html'].str.len()\n",
    "    print(f\"Average HTML length: {html_lengths.mean():,.0f} characters ({html_lengths.mean()/1024:,.1f} KB)\")\n",
    "    print(f\"Max HTML length: {html_lengths.max():,.0f} characters ({html_lengths.max()/1024/1024:,.1f} MB)\")\n",
    "    print(f\"Min HTML length: {html_lengths.min():,.0f} characters ({html_lengths.min()/1024:,.1f} KB)\")\n",
    "    print(f\"Median HTML length: {html_lengths.median():,.0f} characters ({html_lengths.median()/1024:,.1f} KB)\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 6: TARGET VARIABLE ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find target column\n",
    "target_col = None\n",
    "for col in train_df.columns:\n",
    "    if col.lower() in ['label', 'target', 'is_phishing', 'phishing', 'class']:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col:\n",
    "    print(f\"\\nTarget column found: '{target_col}'\")\n",
    "    print(f\"\\nClass distribution (Train sample):\")\n",
    "    print(train_df[target_col].value_counts())\n",
    "    print(f\"\\nClass distribution (% in sample):\")\n",
    "    print(train_df[target_col].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    print(f\"\\nClass distribution (Test sample):\")\n",
    "    print(test_df[target_col].value_counts())\n",
    "    \n",
    "    # Visualize class distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    train_df[target_col].value_counts().plot(kind='bar', color=['#2ecc71', '#e74c3c'])\n",
    "    plt.title('Class Distribution - Train Sample', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Class', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    test_df[target_col].value_counts().plot(kind='bar', color=['#2ecc71', '#e74c3c'])\n",
    "    plt.title('Class Distribution - Test Sample', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Class', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Target column not found. Inspecting column values...\")\n",
    "    for col in train_df.columns:\n",
    "        if col != 'html':\n",
    "            print(f\"\\n{col}: {train_df[col].unique()[:5]}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 7: CHECK FOR MISSING VALUES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "missing_train = train_df.isnull().sum()\n",
    "missing_test = test_df.isnull().sum()\n",
    "\n",
    "print(\"\\nMissing values in TRAIN sample:\")\n",
    "if missing_train.sum() > 0:\n",
    "    print(missing_train[missing_train > 0])\n",
    "    print(f\"\\nMissing value percentages:\")\n",
    "    missing_pct = (missing_train / len(train_df)) * 100\n",
    "    print(missing_pct[missing_pct > 0])\n",
    "else:\n",
    "    print(\"No missing values!\")\n",
    "\n",
    "print(\"\\nMissing values in TEST sample:\")\n",
    "print(missing_test[missing_test > 0] if missing_test.sum() > 0 else \"No missing values!\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 8: CHECK FOR DUPLICATES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DUPLICATE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_duplicates_train = train_df.duplicated().sum()\n",
    "n_duplicates_test = test_df.duplicated().sum()\n",
    "\n",
    "print(f\"\\nDuplicate rows in TRAIN sample: {n_duplicates_train}\")\n",
    "print(f\"Duplicate rows in TEST sample: {n_duplicates_test}\")\n",
    "\n",
    "if n_duplicates_train > 0:\n",
    "    print(f\"   â†’ {(n_duplicates_train/len(train_df)*100):.2f}% of train sample\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 9: BASIC STATISTICS FOR NUMERICAL COLUMNS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NUMERICAL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "numerical_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "if numerical_cols:\n",
    "    print(train_df[numerical_cols].describe())\n",
    "else:\n",
    "    print(\"No numerical columns found (all are text/categorical)\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 10: TEXT/URL COLUMN ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN DETAILS (excluding HTML)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for col in train_df.columns:\n",
    "    if col != 'html':  # Skip HTML for readability\n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        print(f\"   Type: {train_df[col].dtype}\")\n",
    "        print(f\"   Unique values: {train_df[col].nunique():,}\")\n",
    "        print(f\"   Sample values:\")\n",
    "        for i, val in enumerate(train_df[col].head(3), 1):\n",
    "            val_str = str(val)[:100] + \"...\" if len(str(val)) > 100 else str(val)\n",
    "            print(f\"      {i}. {val_str}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 11: URL ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"URL PATTERN ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'url' in train_df.columns:\n",
    "    print(\"\\nURL Statistics:\")\n",
    "    url_lengths = train_df['url'].str.len()\n",
    "    print(f\"   Average URL length: {url_lengths.mean():.1f} characters\")\n",
    "    print(f\"   Max URL length: {url_lengths.max()} characters\")\n",
    "    print(f\"   Min URL length: {url_lengths.min()} characters\")\n",
    "    \n",
    "    # Check for HTTPS vs HTTP\n",
    "    https_count = train_df['url'].str.contains('https://', case=False, na=False).sum()\n",
    "    http_count = train_df['url'].str.contains('http://', case=False, na=False).sum()\n",
    "    print(f\"\\nProtocol Distribution:\")\n",
    "    print(f\"   HTTPS: {https_count} ({https_count/len(train_df)*100:.1f}%)\")\n",
    "    print(f\"   HTTP: {http_count} ({http_count/len(train_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Show some phishing vs legitimate URL examples\n",
    "    if target_col:\n",
    "        print(f\"\\nSample PHISHING URLs:\")\n",
    "        phish_urls = train_df[train_df[target_col] == 'phish']['url'].head(3)\n",
    "        for i, url in enumerate(phish_urls, 1):\n",
    "            print(f\"   {i}. {url}\")\n",
    "        \n",
    "        print(f\"\\nSample LEGITIMATE URLs:\")\n",
    "        legit_urls = train_df[train_df[target_col] == 'benign']['url'].head(3)\n",
    "        for i, url in enumerate(legit_urls, 1):\n",
    "            print(f\"   {i}. {url}\")\n",
    "\n",
    "# Convert date to datetime\n",
    "train_df['date_dt'] = pd.to_datetime(train_df['date'])\n",
    "\n",
    "# Check distribution by label\n",
    "print(\"=\"*60)\n",
    "print(\"DATE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPhishing sites by month:\")\n",
    "print(train_df[train_df['label'] == 'phish']['date_dt'].dt.month.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nBenign sites by month:\")\n",
    "print(train_df[train_df['label'] == 'benign']['date_dt'].dt.month.value_counts().sort_index())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "train_df[train_df['label'] == 'phish']['date_dt'].dt.month.value_counts().sort_index().plot(kind='bar', ax=axes[0], color='red')\n",
    "axes[0].set_title('Phishing Sites by Month')\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "train_df[train_df['label'] == 'benign']['date_dt'].dt.month.value_counts().sort_index().plot(kind='bar', ax=axes[1], color='green')\n",
    "axes[1].set_title('Benign Sites by Month')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# STEP 12: SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EDA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "Dataset loaded successfully\n",
    "Full dataset size: {len(dataset['train']) + len(dataset['test']):,} samples\n",
    "Working with: {len(train_df) + len(test_df):,} samples for EDA\n",
    "Features: {len(train_df.columns)}\n",
    "Target column: {target_col if target_col else 'To be identified'}\n",
    "Missing values: {'Yes' if missing_train.sum() > 0 else 'No'}\n",
    "Duplicates: {'Yes' if n_duplicates_train > 0 else 'No'}\n",
    "HTML column: ~{train_df['html'].str.len().mean()/1024:.0f} KB per sample (HUGE!)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78733446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in your EDA notebook\n",
    "print(\"=\"*60)\n",
    "print(\"BRAND DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get brand counts (excluding None)\n",
    "brand_counts = train_df[train_df['target'].notna()]['target'].value_counts()\n",
    "\n",
    "print(f\"\\nTotal unique brands: {len(brand_counts)}\")\n",
    "print(f\"\\nTop 20 brands:\")\n",
    "print(brand_counts.head(20))\n",
    "\n",
    "print(f\"\\nðŸ“Š Coverage Analysis:\")\n",
    "top_5_coverage = brand_counts.head(5).sum() / brand_counts.sum() * 100\n",
    "top_10_coverage = brand_counts.head(10).sum() / brand_counts.sum() * 100\n",
    "top_20_coverage = brand_counts.head(20).sum() / brand_counts.sum() * 100\n",
    "\n",
    "print(f\"Top 5 brands cover: {top_5_coverage:.1f}% of phishing\")\n",
    "print(f\"Top 10 brands cover: {top_10_coverage:.1f}% of phishing\")\n",
    "print(f\"Top 20 brands cover: {top_20_coverage:.1f}% of phishing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEST: Does language correlate with phishing?\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LANGUAGE vs PHISHING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate phishing rate by language\n",
    "lang_phishing = train_df.groupby('lang')['label'].apply(\n",
    "    lambda x: (x == 'phish').sum() / len(x) * 100\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n% Phishing by Language (Top 10):\")\n",
    "print(lang_phishing.head(10))\n",
    "\n",
    "print(f\"\\nðŸ“Š Overall phishing rate: {(train_df['label'] == 'phish').sum() / len(train_df) * 100:.2f}%\")\n",
    "\n",
    "# Check if languages differ significantly\n",
    "print(\"\\nðŸ” Do languages differ from overall rate?\")\n",
    "for lang, rate in lang_phishing.head(10).items():\n",
    "    overall_rate = (train_df['label'] == 'phish').sum() / len(train_df) * 100\n",
    "    diff = abs(rate - overall_rate)\n",
    "    print(f\"   {lang}: {rate:.1f}% (diff: {diff:.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "lang_phishing.head(15).plot(kind='barh')\n",
    "plt.axvline((train_df['label'] == 'phish').sum() / len(train_df) * 100, \n",
    "            color='red', linestyle='--', label='Overall avg')\n",
    "plt.xlabel('% Phishing')\n",
    "plt.title('Phishing Rate by Language')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPREHENSIVE HTML ANALYSIS - 5000 SAMPLES\n",
    "# ============================================\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYZING HTML PATTERNS - 5000 SAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Take 5000 samples from train set\n",
    "ANALYSIS_SIZE = 5000\n",
    "analysis_df = train_df.head(ANALYSIS_SIZE).copy()\n",
    "\n",
    "print(f\"\\nAnalyzing {len(analysis_df):,} samples...\")\n",
    "print(f\"   Phishing: {(analysis_df['label'] == 'phish').sum():,}\")\n",
    "print(f\"   Legitimate: {(analysis_df['label'] == 'benign').sum():,}\")\n",
    "\n",
    "# Function to extract HTML features\n",
    "def extract_html_stats(html):\n",
    "    \"\"\"Extract HTML features for analysis\"\"\"\n",
    "    if not html or len(html) == 0:\n",
    "        return {\n",
    "            'num_links': 0,\n",
    "            'num_forms': 0,\n",
    "            'num_input_fields': 0,\n",
    "            'has_password_field': 0,\n",
    "            'num_iframes': 0,\n",
    "            'num_images': 0,\n",
    "            'num_scripts': 0,\n",
    "            'num_external_links': 0,\n",
    "            'title_length': 0,\n",
    "            'html_length': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Extract features\n",
    "        all_links = soup.find_all('a', href=True)\n",
    "        external_links = [link for link in all_links if link.get('href', '').startswith('http')]\n",
    "        title_text = soup.title.string if soup.title else ''\n",
    "        \n",
    "        return {\n",
    "            'num_links': len(soup.find_all('a')),\n",
    "            'num_forms': len(soup.find_all('form')),\n",
    "            'num_input_fields': len(soup.find_all('input')),\n",
    "            'has_password_field': 1 if len(soup.find_all('input', {'type': 'password'})) > 0 else 0,\n",
    "            'num_iframes': len(soup.find_all('iframe')),\n",
    "            'num_images': len(soup.find_all('img')),\n",
    "            'num_scripts': len(soup.find_all('script')),\n",
    "            'num_external_links': len(external_links),\n",
    "            'title_length': len(title_text) if title_text else 0,\n",
    "            'html_length': len(html)\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'num_links': 0,\n",
    "            'num_forms': 0,\n",
    "            'num_input_fields': 0,\n",
    "            'has_password_field': 0,\n",
    "            'num_iframes': 0,\n",
    "            'num_images': 0,\n",
    "            'num_scripts': 0,\n",
    "            'num_external_links': 0,\n",
    "            'title_length': 0,\n",
    "            'html_length': 0\n",
    "        }\n",
    "\n",
    "# Extract HTML features for all samples\n",
    "print(\"\\nExtracting HTML features... (this may take a few minutes)\")\n",
    "html_features_list = []\n",
    "for idx, row in analysis_df.iterrows():\n",
    "    features = extract_html_stats(row['html'])\n",
    "    features['label'] = row['label']\n",
    "    html_features_list.append(features)\n",
    "    \n",
    "    if (idx + 1) % 1000 == 0:\n",
    "        print(f\"   Processed {idx + 1:,} samples...\")\n",
    "\n",
    "html_features_df = pd.DataFrame(html_features_list)\n",
    "\n",
    "print(\"Feature extraction complete!\")\n",
    "\n",
    "# ============================================\n",
    "# STATISTICAL COMPARISON\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL COMPARISON: PHISHING vs LEGITIMATE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "phishing_stats = html_features_df[html_features_df['label'] == 'phish'].describe()\n",
    "legitimate_stats = html_features_df[html_features_df['label'] == 'benign'].describe()\n",
    "\n",
    "feature_cols = [col for col in html_features_df.columns if col != 'label']\n",
    "\n",
    "print(\"\\nMEAN VALUES COMPARISON:\\n\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Phishing (mean)': [phishing_stats[col]['mean'] for col in feature_cols],\n",
    "    'Legitimate (mean)': [legitimate_stats[col]['mean'] for col in feature_cols],\n",
    "})\n",
    "comparison_df['Difference'] = comparison_df['Legitimate (mean)'] - comparison_df['Phishing (mean)']\n",
    "comparison_df['Ratio (Legit/Phish)'] = comparison_df['Legitimate (mean)'] / (comparison_df['Phishing (mean)'] + 0.001)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# ============================================\n",
    "# MEDIAN COMPARISON\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEDIAN VALUES COMPARISON:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_median_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Phishing (median)': [phishing_stats[col]['50%'] for col in feature_cols],\n",
    "    'Legitimate (median)': [legitimate_stats[col]['50%'] for col in feature_cols],\n",
    "})\n",
    "comparison_median_df['Difference'] = comparison_median_df['Legitimate (median)'] - comparison_median_df['Phishing (median)']\n",
    "\n",
    "print(comparison_median_df.to_string(index=False))\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nCreating visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(feature_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    phishing_data = html_features_df[html_features_df['label'] == 'phish'][feature]\n",
    "    legitimate_data = html_features_df[html_features_df['label'] == 'benign'][feature]\n",
    "    \n",
    "    # Create box plots\n",
    "    box_data = [phishing_data, legitimate_data]\n",
    "    bp = ax.boxplot(box_data, labels=['Phishing', 'Legitimate'], patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    bp['boxes'][0].set_facecolor('#e74c3c')\n",
    "    bp['boxes'][1].set_facecolor('#2ecc71')\n",
    "    \n",
    "    ax.set_title(feature, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplots if any\n",
    "for idx in range(len(feature_cols), len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('html_features_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'html_features_comparison.png'\")\n",
    "\n",
    "# ============================================\n",
    "# PASSWORD FIELD ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PASSWORD FIELD ANALYSIS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "phishing_with_password = (html_features_df[html_features_df['label'] == 'phish']['has_password_field'] == 1).sum()\n",
    "legitimate_with_password = (html_features_df[html_features_df['label'] == 'benign']['has_password_field'] == 1).sum()\n",
    "\n",
    "total_phishing = (html_features_df['label'] == 'phish').sum()\n",
    "total_legitimate = (html_features_df['label'] == 'benign').sum()\n",
    "\n",
    "print(f\"Phishing sites with password field: {phishing_with_password}/{total_phishing} ({phishing_with_password/total_phishing*100:.1f}%)\")\n",
    "print(f\"Legitimate sites with password field: {legitimate_with_password}/{total_legitimate} ({legitimate_with_password/total_legitimate*100:.1f}%)\")\n",
    "\n",
    "# ============================================\n",
    "# CORRELATION ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRELATION WITH LABEL:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert label to numeric (phish=1, benign=0)\n",
    "html_features_df['label_numeric'] = (html_features_df['label'] == 'phish').astype(int)\n",
    "\n",
    "correlations = html_features_df[feature_cols + ['label_numeric']].corr()['label_numeric'].sort_values(ascending=False)\n",
    "print(\"\\nFeature correlations with phishing label:\")\n",
    "print(correlations[:-1])  # Exclude label_numeric itself\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"- Features with HIGH positive correlation â†’ More common in PHISHING\")\n",
    "print(\"- Features with HIGH negative correlation â†’ More common in LEGITIMATE\")\n",
    "print(\"- Features close to 0 â†’ NOT useful for classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
