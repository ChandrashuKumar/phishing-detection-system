{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd9a407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CNN-LSTM HYBRID - MULTI-MODAL PHISHING DETECTION\n",
      "URL + HTML CONTENT\n",
      "============================================================\n",
      "Started at: 2025-10-26 23:18:42\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CNN-LSTM HYBRID - MULTI-MODAL PHISHING DETECTION\n",
    "# URL + HTML CONTENT\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CNN-LSTM HYBRID - MULTI-MODAL PHISHING DETECTION\")\n",
    "print(\"URL + HTML CONTENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ae38b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GPU SETUP\n",
      "============================================================\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU: NVIDIA GeForce GTX 1650\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GPU SETUP\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU SETUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"WARNING: GPU not available, using CPU (will be slow!)\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8685eef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING PHRESHPHISH DATASET\n",
      "============================================================\n",
      "Loading dataset from cache...\n",
      "Train samples: 371,941\n",
      "Test samples: 36,787\n",
      "\n",
      "Extracting URLs and labels...\n",
      "âœ… Done! 371,941 train, 36,787 test\n",
      "\n",
      "Label distribution (train):\n",
      "  benign: 253,189 (68.1%)\n",
      "  phish: 118,752 (31.9%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD PHRESHPHISH DATASET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING PHRESHPHISH DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading dataset from cache...\")\n",
    "dataset = load_dataset(\"phreshphish/phreshphish\", cache_dir='E:/.cache/huggingface')\n",
    "\n",
    "print(f\"Train samples: {len(dataset['train']):,}\")\n",
    "print(f\"Test samples: {len(dataset['test']):,}\")\n",
    "\n",
    "# Extract URLs and labels (column access - instant!)\n",
    "print(\"\\nExtracting URLs and labels...\")\n",
    "train_urls = dataset['train']['url']\n",
    "train_labels = dataset['train']['label']\n",
    "test_urls = dataset['test']['url']\n",
    "test_labels = dataset['test']['label']\n",
    "\n",
    "print(f\"âœ… Done! {len(train_urls):,} train, {len(test_urls):,} test\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_labels)\n",
    "y_test = le.transform(test_labels)\n",
    "\n",
    "print(f\"\\nLabel distribution (train):\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    label_name = 'benign' if label == 0 else 'phish'\n",
    "    print(f\"  {label_name}: {count:,} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d43704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HTML PREPROCESSING\n",
      "============================================================\n",
      "\n",
      "Found cached cleaned HTML! Loading from disk...\n",
      "Loaded train_html_cleaned: 371,941 samples\n",
      "Loaded test_html_cleaned: 36,787 samples\n",
      "Sample: Ã‡oinbase Pro: Login | ð˜¾oinbase Sign In Ã‡oinbase Pro: Login | ð˜¾oinbase Sign In ð˜¾oinbase Sign In More Search Ctrl + K Ã‡oinbase Pro: Login Powered by Git...\n",
      "\n",
      "Average HTML length: 985 chars\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# HTML PREPROCESSING (WITH CACHING)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HTML PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Paths for cached cleaned HTML\n",
    "CACHE_DIR = '../../../data/processed/url-detection/'\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_HTML_CLEANED_PATH = os.path.join(CACHE_DIR, 'train_html_cleaned.pkl')\n",
    "TEST_HTML_CLEANED_PATH = os.path.join(CACHE_DIR, 'test_html_cleaned.pkl')\n",
    "\n",
    "# Check if cached cleaned HTML exists\n",
    "if os.path.exists(TRAIN_HTML_CLEANED_PATH) and os.path.exists(TEST_HTML_CLEANED_PATH):\n",
    "    print(\"\\nFound cached cleaned HTML! Loading from disk...\")\n",
    "    \n",
    "    with open(TRAIN_HTML_CLEANED_PATH, 'rb') as f:\n",
    "        train_html_cleaned = pickle.load(f)\n",
    "    \n",
    "    with open(TEST_HTML_CLEANED_PATH, 'rb') as f:\n",
    "        test_html_cleaned = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded train_html_cleaned: {len(train_html_cleaned):,} samples\")\n",
    "    print(f\"Loaded test_html_cleaned: {len(test_html_cleaned):,} samples\")\n",
    "    print(f\"Sample: {train_html_cleaned[0][:150]}...\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo cached data found. Processing HTML from scratch...\")\n",
    "    print(\"This will take 20-30 minutes...\")\n",
    "    \n",
    "    def clean_html(html):\n",
    "        \"\"\"Clean HTML content for tokenization\"\"\"\n",
    "        if not html or len(html) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            if len(html) > 50000:\n",
    "                html = html[:50000]\n",
    "            \n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            \n",
    "            for tag in soup(['script', 'style', 'noscript', 'svg']):\n",
    "                tag.decompose()\n",
    "            \n",
    "            text = soup.get_text(separator=' ', strip=True)\n",
    "            \n",
    "            forms = soup.find_all('form')[:3]\n",
    "            form_actions = ' '.join([form.get('action', '')[:50] for form in forms])\n",
    "            \n",
    "            links = soup.find_all('a')[:5]\n",
    "            link_hrefs = ' '.join([link.get('href', '')[:30] for link in links])\n",
    "            \n",
    "            title = soup.title.string if soup.title else \"\"\n",
    "            if title and len(title) > 100:\n",
    "                title = title[:100]\n",
    "            \n",
    "            combined = f\"{title} {text[:5000]} {form_actions} {link_hrefs}\"\n",
    "            combined = re.sub(r'\\s+', ' ', combined).strip()\n",
    "            \n",
    "            return combined[:10000]\n",
    "            \n",
    "        except Exception as e:\n",
    "            return \"\"\n",
    "    \n",
    "    # Process train HTML\n",
    "    train_html_cleaned = []\n",
    "    for i in tqdm(range(0, len(dataset['train']), 1000), desc=\"Train HTML\"):\n",
    "        batch_end = min(i + 1000, len(dataset['train']))\n",
    "        batch_html = dataset['train'][i:batch_end]['html']\n",
    "        batch_cleaned = [clean_html(html) for html in batch_html]\n",
    "        train_html_cleaned.extend(batch_cleaned)\n",
    "        \n",
    "        import gc\n",
    "        gc.collect()\n",
    "    \n",
    "    # Process test HTML\n",
    "    test_html_cleaned = []\n",
    "    for i in tqdm(range(0, len(dataset['test']), 1000), desc=\"Test HTML\"):\n",
    "        batch_end = min(i + 1000, len(dataset['test']))\n",
    "        batch_html = dataset['test'][i:batch_end]['html']\n",
    "        batch_cleaned = [clean_html(html) for html in batch_html]\n",
    "        test_html_cleaned.extend(batch_cleaned)\n",
    "        \n",
    "        import gc\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\nâœ… HTML cleaned!\")\n",
    "    print(f\"Train: {len(train_html_cleaned):,}\")\n",
    "    print(f\"Test: {len(test_html_cleaned):,}\")\n",
    "    \n",
    "    # Save to disk for future use\n",
    "    print(\"\\nSaving cleaned HTML to disk...\")\n",
    "    \n",
    "    with open(TRAIN_HTML_CLEANED_PATH, 'wb') as f:\n",
    "        pickle.dump(train_html_cleaned, f)\n",
    "    \n",
    "    with open(TEST_HTML_CLEANED_PATH, 'wb') as f:\n",
    "        pickle.dump(test_html_cleaned, f)\n",
    "    \n",
    "    print(f\"Saved to {CACHE_DIR}\")\n",
    "\n",
    "print(f\"\\nAverage HTML length: {np.mean([len(h) for h in train_html_cleaned[:1000]]):.0f} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6d01ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HTML TOKENIZATION\n",
      "============================================================\n",
      "Building HTML vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocab: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 371941/371941 [00:13<00:00, 26918.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML vocabulary size: 10,000 words\n",
      "Sample words: ['<PAD>', '<UNK>', 'the', 'to', 'and', 'of', 'a', 'in', 'your', '-', 'for', 'you', '|', 'is', 'with', '&', 'on', 'or', 'by', 'this']\n",
      "Max HTML length: 2000 tokens\n",
      "\n",
      "Tokenizing HTML...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 371941/371941 [00:24<00:00, 15343.32it/s]\n",
      "Test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36787/36787 [00:04<00:00, 8904.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HTML tokenized shapes:\n",
      "X_train_html: (371941, 2000)\n",
      "X_test_html: (36787, 2000)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# HTML TOKENIZATION (WORD-LEVEL)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HTML TOKENIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build HTML vocabulary\n",
    "def build_html_vocab(html_texts, max_vocab=10000):\n",
    "    \"\"\"\n",
    "    Build vocabulary from HTML text\n",
    "    \n",
    "    WHAT: Create mapping from words to indices\n",
    "    WHY: Neural networks need numbers, not words\n",
    "    HOW: Count word frequencies, keep top max_vocab words\n",
    "    \"\"\"\n",
    "    word_counter = Counter()\n",
    "    \n",
    "    for text in tqdm(html_texts, desc=\"Building vocab\"):\n",
    "        words = text.lower().split()\n",
    "        word_counter.update(words)\n",
    "    \n",
    "    # Get most common words\n",
    "    most_common = word_counter.most_common(max_vocab - 2)\n",
    "    \n",
    "    # Create vocabulary\n",
    "    word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for idx, (word, count) in enumerate(most_common, start=2):\n",
    "        word_to_idx[word] = idx\n",
    "    \n",
    "    return word_to_idx\n",
    "\n",
    "print(\"Building HTML vocabulary...\")\n",
    "html_word_to_idx = build_html_vocab(train_html_cleaned, max_vocab=10000)\n",
    "html_vocab_size = len(html_word_to_idx)\n",
    "\n",
    "print(f\"HTML vocabulary size: {html_vocab_size:,} words\")\n",
    "print(f\"Sample words: {list(html_word_to_idx.keys())[:20]}\")\n",
    "\n",
    "# Tokenize HTML\n",
    "def tokenize_html(text, word_to_idx, max_length=2000):\n",
    "    \"\"\"\n",
    "    Convert HTML text to sequence of word indices\n",
    "    \n",
    "    WHAT: text â†’ list of numbers\n",
    "    WHY: Neural networks need fixed-length numerical input\n",
    "    HOW: Split text, map words to indices, pad to max_length\n",
    "    \"\"\"\n",
    "    words = text.lower().split()[:max_length]\n",
    "    tokens = [word_to_idx.get(word, 1) for word in words]  # 1 = <UNK>\n",
    "    \n",
    "    # Pad to max_length\n",
    "    if len(tokens) < max_length:\n",
    "        tokens += [0] * (max_length - len(tokens))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "MAX_HTML_LENGTH = 2000\n",
    "print(f\"Max HTML length: {MAX_HTML_LENGTH} tokens\")\n",
    "\n",
    "print(\"\\nTokenizing HTML...\")\n",
    "X_train_html = np.array([tokenize_html(html, html_word_to_idx, MAX_HTML_LENGTH) \n",
    "                         for html in tqdm(train_html_cleaned, desc=\"Train\")])\n",
    "X_test_html = np.array([tokenize_html(html, html_word_to_idx, MAX_HTML_LENGTH) \n",
    "                        for html in tqdm(test_html_cleaned, desc=\"Test\")])\n",
    "\n",
    "print(f\"\\nHTML tokenized shapes:\")\n",
    "print(f\"X_train_html: {X_train_html.shape}\")\n",
    "print(f\"X_test_html: {X_test_html.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d074662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "URL TOKENIZATION\n",
      "============================================================\n",
      "Converting URL columns to lists...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Convert Dataset columns to Python lists\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConverting URL columns to lists...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m train_urls_list = \u001b[38;5;28mlist\u001b[39m(\u001b[43mtrain_urls\u001b[49m)\n\u001b[32m     12\u001b[39m test_urls_list = \u001b[38;5;28mlist\u001b[39m(test_urls)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Build character vocabulary\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'train_urls' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# URL TOKENIZATION (CHARACTER-LEVEL)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"URL TOKENIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert Dataset columns to Python lists\n",
    "print(\"Converting URL columns to lists...\")\n",
    "train_urls_list = list(train_urls)\n",
    "test_urls_list = list(test_urls)\n",
    "\n",
    "# Build character vocabulary\n",
    "print(\"Building character vocabulary...\")\n",
    "all_urls = train_urls_list + test_urls_list\n",
    "all_chars = set(''.join(all_urls))\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(all_chars))}\n",
    "char_to_idx['<PAD>'] = 0\n",
    "char_vocab_size = len(char_to_idx)\n",
    "\n",
    "print(f\"URL vocabulary size: {char_vocab_size} characters\")\n",
    "print(f\"Sample characters: {list(char_to_idx.keys())[:20]}\")\n",
    "\n",
    "# Tokenize URLs\n",
    "def tokenize_url(url, char_to_idx, max_length=200):\n",
    "    \"\"\"Convert URL to sequence of character indices\"\"\"\n",
    "    tokens = [char_to_idx.get(char, 0) for char in url[:max_length]]\n",
    "    if len(tokens) < max_length:\n",
    "        tokens += [0] * (max_length - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "MAX_URL_LENGTH = 200\n",
    "\n",
    "print(\"\\nTokenizing URLs...\")\n",
    "X_train_url = np.array([tokenize_url(url, char_to_idx, MAX_URL_LENGTH) \n",
    "                        for url in tqdm(train_urls_list, desc=\"Train\")])\n",
    "X_test_url = np.array([tokenize_url(url, char_to_idx, MAX_URL_LENGTH) \n",
    "                       for url in tqdm(test_urls_list, desc=\"Test\")])\n",
    "\n",
    "print(f\"\\nURL tokenized shapes:\")\n",
    "print(f\"X_train_url: {X_train_url.shape}\")\n",
    "print(f\"X_test_url: {X_test_url.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41be0111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING PYTORCH DATASETS\n",
      "============================================================\n",
      "Class weight (phishing): 2.13\n",
      "Batch size: 256\n",
      "Train batches: 1453\n",
      "Test batches: 144\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PYTORCH DATASET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING PYTORCH DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that returns both URL and HTML for each sample\n",
    "    \n",
    "    WHAT: Custom dataset for dual-input model\n",
    "    WHY: PyTorch needs Dataset class to load data efficiently\n",
    "    HOW: Return (url_tokens, html_tokens, label) for each index\n",
    "    \"\"\"\n",
    "    def __init__(self, X_url, X_html, y):\n",
    "        self.X_url = torch.LongTensor(X_url)\n",
    "        self.X_html = torch.LongTensor(X_html)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_url)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_url[idx], self.X_html[idx], self.y[idx]\n",
    "\n",
    "train_dataset = MultiModalDataset(X_train_url, X_train_html, y_train)\n",
    "test_dataset = MultiModalDataset(X_test_url, X_test_html, y_test)\n",
    "\n",
    "# Class weights\n",
    "n_benign = (y_train == 0).sum()\n",
    "n_phish = (y_train == 1).sum()\n",
    "pos_weight = torch.tensor([n_benign / n_phish]).to(device)\n",
    "\n",
    "print(f\"Class weight (phishing): {pos_weight.item():.2f}\")\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b3a8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ATTENTION MECHANISM\n",
    "# ============================================\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism to focus on important parts\n",
    "    \n",
    "    WHAT: Learns which features are most important\n",
    "    WHY: Not all parts of URL/HTML matter equally\n",
    "    HOW: Computes attention weights, applies to features\n",
    "    \n",
    "    Example: \".tk\" gets high weight, \"https\" gets low weight\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_dim)\n",
    "        attention_weights = torch.softmax(self.attention(x), dim=1)\n",
    "        weighted = x * attention_weights\n",
    "        return weighted, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb23310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CNN-LSTM HYBRID ARCHITECTURE\n",
    "# ============================================\n",
    "\n",
    "class CNNLSTMHybrid(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-branch CNN + LSTM for URL and HTML phishing detection\n",
    "    \n",
    "    ARCHITECTURE:\n",
    "    1. URL Branch (CNN): Character-level pattern detection\n",
    "    2. HTML Branch (CNN): Word-level pattern detection\n",
    "    3. Concatenate: Combine both branches\n",
    "    4. LSTM: Sequential understanding of combined features\n",
    "    5. Dense: Final classification\n",
    "    \"\"\"\n",
    "    def __init__(self, char_vocab_size, word_vocab_size):\n",
    "        super(CNNLSTMHybrid, self).__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # URL BRANCH (Character-level CNN)\n",
    "        # ========================================\n",
    "        \n",
    "        self.url_embedding = nn.Embedding(char_vocab_size, 16, padding_idx=0)\n",
    "        \n",
    "        # Multiple kernel sizes to capture different n-gram patterns\n",
    "        self.url_conv1 = nn.Conv1d(16, 256, kernel_size=3, padding=1)\n",
    "        self.url_conv2 = nn.Conv1d(256, 256, kernel_size=5, padding=2)\n",
    "        self.url_conv3 = nn.Conv1d(256, 128, kernel_size=7, padding=3)\n",
    "        \n",
    "        self.url_attention = AttentionLayer(128)\n",
    "        \n",
    "        # ========================================\n",
    "        # HTML BRANCH (Word-level CNN)\n",
    "        # ========================================\n",
    "        \n",
    "        self.html_embedding = nn.Embedding(word_vocab_size, 16, padding_idx=0)\n",
    "        \n",
    "        # Multiple kernel sizes for different phrase patterns\n",
    "        self.html_conv1 = nn.Conv1d(16, 256, kernel_size=3, padding=1)\n",
    "        self.html_conv2 = nn.Conv1d(256, 256, kernel_size=5, padding=2)\n",
    "        self.html_conv3 = nn.Conv1d(256, 128, kernel_size=7, padding=3)\n",
    "        \n",
    "        self.html_attention = AttentionLayer(128)\n",
    "        \n",
    "        # ========================================\n",
    "        # LSTM FUSION (Bidirectional)\n",
    "        # ========================================\n",
    "        \n",
    "        # Input: 128 (URL) + 128 (HTML) = 256\n",
    "        # Bidirectional LSTM doubles the output\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # CLASSIFICATION LAYERS\n",
    "        # ========================================\n",
    "        \n",
    "        # LSTM output: 128 * 2 (bidirectional) = 256\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, url_input, html_input):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \n",
    "        Args:\n",
    "            url_input: (batch, 200) - Character tokens\n",
    "            html_input: (batch, 2000) - Word tokens\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch,) - Phishing probability logits\n",
    "        \"\"\"\n",
    "        \n",
    "        # ========================================\n",
    "        # PROCESS URL (Character CNN)\n",
    "        # ========================================\n",
    "        \n",
    "        # Embed: (batch, 200) â†’ (batch, 200, 16)\n",
    "        url_embed = self.url_embedding(url_input)\n",
    "        \n",
    "        # Transpose for Conv1D: (batch, 16, 200)\n",
    "        url_embed = url_embed.transpose(1, 2)\n",
    "        \n",
    "        # Convolutions\n",
    "        url_conv = self.relu(self.url_conv1(url_embed))  # (batch, 256, 200)\n",
    "        url_conv = self.relu(self.url_conv2(url_conv))   # (batch, 256, 200)\n",
    "        url_conv = self.relu(self.url_conv3(url_conv))   # (batch, 128, 200)\n",
    "        \n",
    "        # Transpose back: (batch, 200, 128)\n",
    "        url_conv = url_conv.transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        url_features, url_attention_weights = self.url_attention(url_conv)\n",
    "        \n",
    "        # Global max pooling: (batch, 128)\n",
    "        url_features = torch.max(url_features, dim=1)[0]\n",
    "        \n",
    "        # ========================================\n",
    "        # PROCESS HTML (Word CNN)\n",
    "        # ========================================\n",
    "        \n",
    "        # Embed: (batch, 2000) â†’ (batch, 2000, 16)\n",
    "        html_embed = self.html_embedding(html_input)\n",
    "        \n",
    "        # Transpose for Conv1D: (batch, 16, 2000)\n",
    "        html_embed = html_embed.transpose(1, 2)\n",
    "        \n",
    "        # Convolutions\n",
    "        html_conv = self.relu(self.html_conv1(html_embed))  # (batch, 256, 2000)\n",
    "        html_conv = self.relu(self.html_conv2(html_conv))   # (batch, 256, 2000)\n",
    "        html_conv = self.relu(self.html_conv3(html_conv))   # (batch, 128, 2000)\n",
    "        \n",
    "        # Transpose back: (batch, 2000, 128)\n",
    "        html_conv = html_conv.transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        html_features, html_attention_weights = self.html_attention(html_conv)\n",
    "        \n",
    "        # Global max pooling: (batch, 128)\n",
    "        html_features = torch.max(html_features, dim=1)[0]\n",
    "        \n",
    "        # ========================================\n",
    "        # COMBINE FEATURES\n",
    "        # ========================================\n",
    "        \n",
    "        # Concatenate URL and HTML features: (batch, 256)\n",
    "        combined = torch.cat([url_features, html_features], dim=1)\n",
    "        \n",
    "        # Expand for LSTM: (batch, 1, 256)\n",
    "        # LSTM expects (batch, sequence, features)\n",
    "        combined = combined.unsqueeze(1)\n",
    "        \n",
    "        # ========================================\n",
    "        # LSTM SEQUENTIAL PROCESSING\n",
    "        # ========================================\n",
    "        \n",
    "        # LSTM: (batch, 1, 256) â†’ (batch, 1, 256)\n",
    "        # 256 = 128 hidden * 2 (bidirectional)\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        \n",
    "        # Remove sequence dimension: (batch, 256)\n",
    "        lstm_out = lstm_out.squeeze(1)\n",
    "        \n",
    "        # ========================================\n",
    "        # CLASSIFICATION\n",
    "        # ========================================\n",
    "        \n",
    "        x = self.relu(self.fc1(lstm_out))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x.squeeze(), url_attention_weights, html_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66d96563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL ARCHITECTURE\n",
      "============================================================\n",
      "\n",
      "Total parameters: 2,135,571\n",
      "Trainable parameters: 2,135,571\n",
      "\n",
      "Model summary:\n",
      "CNNLSTMHybrid(\n",
      "  (url_embedding): Embedding(225, 16, padding_idx=0)\n",
      "  (url_conv1): Conv1d(16, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (url_conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (url_conv3): Conv1d(256, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (url_attention): AttentionLayer(\n",
      "    (attention): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (html_embedding): Embedding(10000, 16, padding_idx=0)\n",
      "  (html_conv1): Conv1d(16, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (html_conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (html_conv3): Conv1d(256, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (html_attention): AttentionLayer(\n",
      "    (attention): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (lstm): LSTM(256, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "\n",
      "Optimizer: Adam (lr=0.001)\n",
      "Loss: BCEWithLogitsLoss (pos_weight=2.13)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# INITIALIZE MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = CNNLSTMHybrid(char_vocab_size, html_vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel summary:\")\n",
    "print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimizer: Adam (lr=0.001)\")\n",
    "print(f\"Loss: BCEWithLogitsLoss (pos_weight={pos_weight.item():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dafa4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_url, batch_html, batch_y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        batch_url = batch_url.to(device)\n",
    "        batch_html = batch_html.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, _, _ = model(batch_url, batch_html)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, f1, recall\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_url, batch_html, batch_y in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            batch_url = batch_url.to(device)\n",
    "            batch_html = batch_html.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            outputs, _, _ = model(batch_url, batch_html)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = probs > 0.5\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, all_preds, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d6888d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING\n",
      "============================================================\n",
      "Training for 10 epochs...\n",
      "Expected time: 2-3 hours with GPU\n",
      "\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.3310, Acc: 89.22%, F1: 83.78%, Recall: 87.22%\n",
      "Val   - Loss: 0.3031, Acc: 92.51%, Prec: 81.14%, Recall: 77.04%, F1: 79.04%\n",
      "Best model saved! (F1: 79.04%)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1275, Acc: 97.03%, F1: 95.37%, Recall: 95.65%\n",
      "Val   - Loss: 0.2010, Acc: 95.74%, Prec: 91.05%, Recall: 85.10%, F1: 87.97%\n",
      "Best model saved! (F1: 87.97%)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0699, Acc: 98.39%, F1: 97.48%, Recall: 97.87%\n",
      "Val   - Loss: 0.1758, Acc: 96.17%, Prec: 90.30%, Recall: 88.59%, F1: 89.44%\n",
      "Best model saved! (F1: 89.44%)\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0511, Acc: 98.81%, F1: 98.15%, Recall: 98.56%\n",
      "Val   - Loss: 0.1936, Acc: 96.48%, Prec: 93.73%, Recall: 86.56%, F1: 90.00%\n",
      "Best model saved! (F1: 90.00%)\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0398, Acc: 99.04%, F1: 98.51%, Recall: 98.95%\n",
      "Val   - Loss: 0.1743, Acc: 96.62%, Prec: 93.78%, Recall: 87.33%, F1: 90.44%\n",
      "Best model saved! (F1: 90.44%)\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0317, Acc: 99.26%, F1: 98.85%, Recall: 99.17%\n",
      "Val   - Loss: 0.1917, Acc: 96.57%, Prec: 91.33%, Recall: 89.84%, F1: 90.57%\n",
      "Best model saved! (F1: 90.57%)\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0265, Acc: 99.35%, F1: 98.98%, Recall: 99.33%\n",
      "Val   - Loss: 0.2670, Acc: 96.75%, Prec: 95.01%, Recall: 86.82%, F1: 90.73%\n",
      "Best model saved! (F1: 90.73%)\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0222, Acc: 99.47%, F1: 99.17%, Recall: 99.44%\n",
      "Val   - Loss: 0.2785, Acc: 96.93%, Prec: 95.92%, Recall: 86.94%, F1: 91.21%\n",
      "Best model saved! (F1: 91.21%)\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0186, Acc: 99.53%, F1: 99.27%, Recall: 99.54%\n",
      "Val   - Loss: 0.2405, Acc: 96.86%, Prec: 94.56%, Recall: 87.94%, F1: 91.13%\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0157, Acc: 99.62%, F1: 99.40%, Recall: 99.62%\n",
      "Val   - Loss: 0.2442, Acc: 96.97%, Prec: 94.76%, Recall: 88.37%, F1: 91.45%\n",
      "Best model saved! (F1: 91.45%)\n",
      "\n",
      "Training complete!\n",
      "Total time: 190.47 minutes\n",
      "Best validation F1: 91.45%\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRAINING LOOP\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "best_f1 = 0\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [], 'train_f1': [], 'train_recall': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_precision': [], 'val_recall': [], 'val_f1': []\n",
    "}\n",
    "\n",
    "print(f\"Training for {NUM_EPOCHS} epochs...\")\n",
    "print(\"Expected time: 2-3 hours with GPU\\n\")\n",
    "training_start = datetime.now()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_f1, train_recall = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc, val_prec, val_recall, val_f1, _, _ = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['train_recall'].append(train_recall)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_precision'].append(val_prec)\n",
    "    history['val_recall'].append(val_recall)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc*100:.2f}%, F1: {train_f1*100:.2f}%, Recall: {train_recall*100:.2f}%\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc*100:.2f}%, Prec: {val_prec*100:.2f}%, Recall: {val_recall*100:.2f}%, F1: {val_f1*100:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), '../../../models/url-detection/cnn_lstm_best.pth')\n",
    "        print(f\"Best model saved! (F1: {best_f1*100:.2f}%)\")\n",
    "\n",
    "training_end = datetime.now()\n",
    "training_time = (training_end - training_start).total_seconds()\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Total time: {training_time/60:.2f} minutes\")\n",
    "print(f\"Best validation F1: {best_f1*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
