{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FEATURE ENGINEERING - URL FEATURE EXTRACTION WITH TF-IDF\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import pickle\n",
    "\n",
    "os.environ['HF_HOME'] = 'E:/.cache/huggingface'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHISHING DETECTION - FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# URL FEATURE EXTRACTION FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def extract_url_features(url):\n",
    "    \"\"\"\n",
    "    Extract 20 features from a URL\n",
    "    \n",
    "    WHAT: Takes a URL string and extracts numerical features\n",
    "    WHY: Machine learning models need numbers, not text\n",
    "    HOW: Parse the URL and count/check various patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize feature dictionary\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Parse URL into components\n",
    "        # WHAT: urlparse breaks URL into parts (scheme, domain, path, etc.)\n",
    "        # WHY: Easier to analyze individual parts\n",
    "        parsed = urlparse(url)\n",
    "        \n",
    "        # Extract components\n",
    "        scheme = parsed.scheme          # http or https\n",
    "        netloc = parsed.netloc          # domain + subdomain\n",
    "        path = parsed.path              # /path/to/page\n",
    "        params = parsed.params          # parameters\n",
    "        query = parsed.query            # ?key=value\n",
    "        \n",
    "        # ========================================\n",
    "        # CATEGORY 1: BASIC LENGTH FEATURES (4)\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature 1: Total URL Length\n",
    "        # WHAT: Count all characters in URL\n",
    "        # WHY: Phishing URLs are often longer to hide intent\n",
    "        # EXAMPLE: \"https://bit.ly/abc\" = 19 chars\n",
    "        features['url_length'] = len(url)\n",
    "        \n",
    "        # Feature 2: Domain Length\n",
    "        # WHAT: Length of domain name only (without subdomain)\n",
    "        # WHY: Legit domains are short and memorable (google.com, amazon.com)\n",
    "        # HOW: Split domain by dots, take last 2 parts (domain + TLD)\n",
    "        domain_parts = netloc.split('.')\n",
    "        if len(domain_parts) >= 2:\n",
    "            domain = domain_parts[-2] + '.' + domain_parts[-1]\n",
    "            features['domain_length'] = len(domain)\n",
    "        else:\n",
    "            features['domain_length'] = len(netloc)\n",
    "        \n",
    "        # Feature 3: Path Length\n",
    "        # WHAT: Length of path after domain\n",
    "        # WHY: Long paths like /verify/account/login/secure can be suspicious\n",
    "        # EXAMPLE: \"/login\" = 6, \"/a/b/c/d/e\" = 9\n",
    "        features['path_length'] = len(path)\n",
    "        \n",
    "        # Feature 4: TLD (Top-Level Domain) Length\n",
    "        # WHAT: Length of the domain extension (.com, .online, .tk)\n",
    "        # WHY: Suspicious TLDs are often longer\n",
    "        # EXAMPLE: .com = 3, .online = 6\n",
    "        tld = domain_parts[-1] if domain_parts else ''\n",
    "        features['tld_length'] = len(tld)\n",
    "        \n",
    "        # ========================================\n",
    "        # CATEGORY 2: CHARACTER COUNT FEATURES (7)\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature 5: Number of Dots\n",
    "        # WHAT: Count all '.' in URL\n",
    "        # WHY: More dots = more subdomains = potentially suspicious\n",
    "        # EXAMPLE: \"a.b.c.example.com\" = 4 dots\n",
    "        features['num_dots'] = url.count('.')\n",
    "        \n",
    "        # Feature 6: Number of Hyphens\n",
    "        # WHAT: Count all '-' in URL\n",
    "        # WHY: Phishers use hyphens to mimic brands (pay-pal instead of paypal)\n",
    "        # EXAMPLE: \"secure-login-verify.com\" = 2 hyphens\n",
    "        features['num_hyphens'] = url.count('-')\n",
    "        \n",
    "        # Feature 7: Number of Underscores\n",
    "        # WHAT: Count all '_' in URL\n",
    "        # WHY: Rare in legitimate URLs, more common in phishing\n",
    "        # EXAMPLE: \"fake_login_page.com\" = 2 underscores\n",
    "        features['num_underscores'] = url.count('_')\n",
    "        \n",
    "        # Feature 8: Number of Slashes\n",
    "        # WHAT: Count all '/' in URL\n",
    "        # WHY: Deep directory structures can hide malicious pages\n",
    "        # EXAMPLE: \"/a/b/c/d\" = 4 slashes\n",
    "        features['num_slashes'] = url.count('/')\n",
    "        \n",
    "        # Feature 9: Number of Question Marks\n",
    "        # WHAT: Count all '?' in URL\n",
    "        # WHY: Multiple question marks are unusual/suspicious\n",
    "        # EXAMPLE: \"?redirect=?url=\" = 2 question marks\n",
    "        features['num_question'] = url.count('?')\n",
    "        \n",
    "        # Feature 10: Number of Ampersands\n",
    "        # WHAT: Count all '&' in URL\n",
    "        # WHY: Many parameters can indicate tracking or redirection\n",
    "        # EXAMPLE: \"?a=1&b=2&c=3\" = 2 ampersands\n",
    "        features['num_ampersand'] = url.count('&')\n",
    "        \n",
    "        # Feature 11: Number of @ Symbols\n",
    "        # WHAT: Count all '@' in URL\n",
    "        # WHY: MAJOR RED FLAG! Used to trick users\n",
    "        # EXAMPLE: \"http://paypal.com@attacker.com\" redirects to attacker.com\n",
    "        # The @ symbol makes browser ignore everything before it\n",
    "        features['num_at'] = url.count('@')\n",
    "        \n",
    "        # ========================================\n",
    "        # CATEGORY 3: SUSPICIOUS PATTERNS (5)\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature 12: Has IP Address Instead of Domain\n",
    "        # WHAT: Check if domain is an IP address (192.168.1.1)\n",
    "        # WHY: Legitimate sites use domain names, not raw IPs\n",
    "        # HOW: Use regex to match IP pattern\n",
    "        ip_pattern = r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'\n",
    "        features['has_ip_address'] = 1 if re.search(ip_pattern, netloc) else 0\n",
    "        \n",
    "        # Feature 13: Is HTTPS (Secure)\n",
    "        # WHAT: Check if URL uses HTTPS protocol\n",
    "        # WHY: Most legitimate sites use HTTPS in 2024\n",
    "        # HOW: Check scheme\n",
    "        features['is_https'] = 1 if scheme == 'https' else 0\n",
    "        \n",
    "        # Feature 14: Number of Subdomains\n",
    "        # WHAT: Count how many subdomains exist\n",
    "        # WHY: Too many subdomains can be suspicious\n",
    "        # EXAMPLE: \"a.b.c.example.com\" = 3 subdomains (a, b, c)\n",
    "        # HOW: Count domain parts minus 2 (domain + TLD)\n",
    "        features['num_subdomains'] = max(0, len(domain_parts) - 2)\n",
    "        \n",
    "        # Feature 15: Has Non-Standard Port\n",
    "        # WHAT: Check if URL specifies a port number\n",
    "        # WHY: Standard ports (80, 443) are usually omitted\n",
    "        # EXAMPLE: \"example.com:8080\" has port, \"example.com\" doesn't\n",
    "        # Better version\n",
    "        # IPv6 address - check if there's a colon AFTER the closing bracket\n",
    "        if netloc.startswith('['):\n",
    "            features['has_port'] = 1 if ']:' in netloc else 0\n",
    "        # Regular domain - check for colon\n",
    "        else:\n",
    "            features['has_port'] = 1 if ':' in netloc else 0\n",
    "        \n",
    "        # Feature 16: Number of Digits in Domain\n",
    "        # WHAT: Count numerical digits in domain name\n",
    "        # WHY: Legitimate brands rarely use numbers (except l33t sp34k phishing)\n",
    "        # EXAMPLE: \"paypa1.com\" (1 instead of l) = typosquatting\n",
    "        features['num_digits_domain'] = sum(c.isdigit() for c in netloc)\n",
    "        \n",
    "        # ========================================\n",
    "        # CATEGORY 4: SPECIAL CHECKS (4)\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature 17: Suspicious TLD\n",
    "        # WHAT: Check if TLD is commonly used for phishing\n",
    "        # WHY: Free TLDs (.tk, .ml, .ga) are heavily abused\n",
    "        # LIST: Based on research of phishing-prone TLDs\n",
    "        suspicious_tlds = [\n",
    "            'tk', 'ml', 'ga', 'cf', 'gq',  # Free Freenom TLDs\n",
    "            'zip', 'review', 'country', 'kim', 'science',\n",
    "            'work', 'party', 'gdn', 'link'\n",
    "        ]\n",
    "        features['suspicious_tld'] = 1 if tld.lower() in suspicious_tlds else 0\n",
    "        \n",
    "        # Feature 18: Uses Free Hosting\n",
    "        # WHAT: Check if URL uses free hosting service\n",
    "        # WHY: Phishers love free platforms (no cost, easy setup)\n",
    "        # LIST: Common free hosting services\n",
    "        free_hosting_indicators = [\n",
    "            'github.io', 'gitlab.io', 'bitbucket.io',\n",
    "            'netlify.app', 'vercel.app', 'web.app',\n",
    "            'firebaseapp.com', 'herokuapp.com',\n",
    "            'wordpress.com', 'wixsite.com', 'weebly.com',\n",
    "            'blogspot.com', 'tumblr.com',\n",
    "            'pages.dev', 'azurewebsites.net',\n",
    "            '000webhostapp.com', 'freehosting.com',\n",
    "            'googlepages.com', 'gitbook.io'\n",
    "        ]\n",
    "        features['free_hosting'] = 1 if any(indicator in netloc.lower() for indicator in free_hosting_indicators) else 0\n",
    "        \n",
    "        # Feature 19: URL Entropy (Randomness)\n",
    "        # WHAT: Calculate Shannon entropy (measure of randomness)\n",
    "        # WHY: Random strings (asdk2j3k.com) have high entropy\n",
    "        #      Real words (amazon.com) have lower entropy\n",
    "        # HOW: Calculate probability distribution of characters\n",
    "        # MATH: Entropy = -Σ(p(x) * log2(p(x)))\n",
    "        def calculate_entropy(text):\n",
    "            if not text:\n",
    "                return 0\n",
    "            # Count character frequencies\n",
    "            counter = Counter(text)\n",
    "            length = len(text)\n",
    "            # Calculate entropy\n",
    "            entropy = 0\n",
    "            for count in counter.values():\n",
    "                probability = count / length\n",
    "                if probability > 0:\n",
    "                    entropy -= probability * math.log2(probability)\n",
    "            return entropy\n",
    "        \n",
    "        features['url_entropy'] = calculate_entropy(url)\n",
    "        \n",
    "        # Feature 20: Has WWW Prefix\n",
    "        # WHAT: Check if domain starts with 'www.'\n",
    "        # WHY: Many legitimate sites use www, phishing sometimes skips it\n",
    "        # NOTE: Not a strong signal alone, but useful in combination\n",
    "        features['has_www'] = 1 if netloc.startswith('www.') else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If URL parsing fails, return zeros for all features\n",
    "        # WHAT: Handle malformed URLs gracefully\n",
    "        # WHY: Some URLs might be invalid/corrupted\n",
    "        print(f\"Error parsing URL: {url[:50]}... Error: {str(e)}\")\n",
    "        features = {\n",
    "            'url_length': 0, 'domain_length': 0, 'path_length': 0, 'tld_length': 0,\n",
    "            'num_dots': 0, 'num_hyphens': 0, 'num_underscores': 0, 'num_slashes': 0,\n",
    "            'num_question': 0, 'num_ampersand': 0, 'num_at': 0,\n",
    "            'has_ip_address': 0, 'is_https': 0, 'num_subdomains': 0, 'has_port': 0,\n",
    "            'num_digits_domain': 0, 'suspicious_tld': 0, 'free_hosting': 0,\n",
    "            'url_entropy': 0, 'has_www': 0\n",
    "        }\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"URL feature extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD DATASET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nLoading PhreshPhish dataset...\")\n",
    "dataset = load_dataset(\"phreshphish/phreshphish\", cache_dir='E:/.cache/huggingface')\n",
    "\n",
    "print(f\"Train: {len(dataset['train']):,} samples\")\n",
    "print(f\"Test: {len(dataset['test']):,} samples\")\n",
    "\n",
    "# ============================================\n",
    "# FIT TF-IDF VECTORIZER ON TRAINING URLS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nFitting TF-IDF vectorizer on training URLs...\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=500,\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "train_urls = [dataset['train'][i]['url'] for i in range(len(dataset['train']))]\n",
    "tfidf_vectorizer.fit(train_urls)\n",
    "\n",
    "print(f\"TF-IDF vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Save vectorizer for later use\n",
    "vectorizer_path = '../../../models/url-detection/tfidf_vectorizer.pkl'\n",
    "os.makedirs(os.path.dirname(vectorizer_path), exist_ok=True)\n",
    "with open(vectorizer_path, 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "print(f\"TF-IDF vectorizer saved to {vectorizer_path}\")\n",
    "\n",
    "# ============================================\n",
    "# BATCH PROCESSING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def process_batch_with_tfidf(batch_data, vectorizer):\n",
    "    df = pd.DataFrame(batch_data)\n",
    "    \n",
    "    # Extract manual URL features\n",
    "    url_features_list = [extract_url_features(url) for url in df['url']]\n",
    "    url_features_df = pd.DataFrame(url_features_list)\n",
    "    \n",
    "    # Extract TF-IDF features\n",
    "    tfidf_features = vectorizer.transform(df['url'])\n",
    "    \n",
    "    # Combine manual features (dense) with TF-IDF (sparse)\n",
    "    manual_features_sparse = csr_matrix(url_features_df.values)\n",
    "    combined_features = hstack([manual_features_sparse, tfidf_features])\n",
    "    \n",
    "    # Create column names\n",
    "    manual_cols = url_features_df.columns.tolist()\n",
    "    tfidf_cols = [f'tfidf_{i}' for i in range(tfidf_features.shape[1])]\n",
    "    all_cols = manual_cols + tfidf_cols\n",
    "    \n",
    "    # Convert to dense DataFrame\n",
    "    combined_df = pd.DataFrame(combined_features.toarray(), columns=all_cols)\n",
    "    combined_df['label'] = df['label'].values\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# ============================================\n",
    "# PROCESS TRAIN SET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING TRAIN SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "TRAIN_OUTPUT_FILE = '../../../data/processed/url-detection/phishing_features_train.csv'\n",
    "\n",
    "num_train_batches = (len(dataset['train']) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in tqdm(range(num_train_batches), desc=\"Processing train batches\"):\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(dataset['train']))\n",
    "    \n",
    "    batch = dataset['train'].select(range(start_idx, end_idx))\n",
    "    batch_dict = {\n",
    "        'url': batch['url'],\n",
    "        'label': batch['label']\n",
    "    }\n",
    "    \n",
    "    processed_batch = process_batch_with_tfidf(batch_dict, tfidf_vectorizer)\n",
    "    \n",
    "    if i == 0:\n",
    "        processed_batch.to_csv(TRAIN_OUTPUT_FILE, index=False, mode='w')\n",
    "    else:\n",
    "        processed_batch.to_csv(TRAIN_OUTPUT_FILE, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Train set saved to {TRAIN_OUTPUT_FILE}\")\n",
    "\n",
    "# ============================================\n",
    "# PROCESS TEST SET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "TEST_OUTPUT_FILE = '../../../data/processed/url-detection/phishing_features_test.csv'\n",
    "\n",
    "num_test_batches = (len(dataset['test']) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in tqdm(range(num_test_batches), desc=\"Processing test batches\"):\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(dataset['test']))\n",
    "    \n",
    "    batch = dataset['test'].select(range(start_idx, end_idx))\n",
    "    batch_dict = {\n",
    "        'url': batch['url'],\n",
    "        'label': batch['label']\n",
    "    }\n",
    "    \n",
    "    processed_batch = process_batch_with_tfidf(batch_dict, tfidf_vectorizer)\n",
    "    \n",
    "    if i == 0:\n",
    "        processed_batch.to_csv(TEST_OUTPUT_FILE, index=False, mode='w')\n",
    "    else:\n",
    "        processed_batch.to_csv(TEST_OUTPUT_FILE, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Test set saved to {TEST_OUTPUT_FILE}\")\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train file: {TRAIN_OUTPUT_FILE}\")\n",
    "print(f\"  Rows: {len(dataset['train']):,}\")\n",
    "print(f\"  Columns: {20 + 500 + 1} (20 manual + 500 TF-IDF + 1 label)\")\n",
    "print(f\"\\nTest file: {TEST_OUTPUT_FILE}\")\n",
    "print(f\"  Rows: {len(dataset['test']):,}\")\n",
    "print(f\"  Columns: {20 + 500 + 1}\")\n",
    "print(f\"\\nTF-IDF vectorizer: {vectorizer_path}\")\n",
    "print(\"\\nReady for model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734df02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BATCH PROCESSING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def process_batch(batch_data):\n",
    "    \"\"\"\n",
    "    Process a batch of data and extract all features\n",
    "    \n",
    "    WHAT: Takes a batch of rows, extracts features, returns DataFrame\n",
    "    WHY: Can't load 408K rows at once (memory crash)\n",
    "    HOW: Process 10K rows at a time\n",
    "    \n",
    "    Input: Dictionary with columns from dataset\n",
    "    Output: DataFrame with extracted features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert batch to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(batch_data)\n",
    "    \n",
    "    # Extract URL features for each row\n",
    "    # WHAT: Apply feature extraction to every URL\n",
    "    # HOW: Use list comprehension to process all rows\n",
    "    print(\"   Extracting URL features...\")\n",
    "    url_features_list = [extract_url_features(url) for url in df['url']]\n",
    "    url_features_df = pd.DataFrame(url_features_list)\n",
    "    \n",
    "    # Keep label (target variable)\n",
    "    df['label'] = df['label']\n",
    "    \n",
    "    # Combine all features\n",
    "    # WHAT: Merge URL features with label\n",
    "    # HOW: Concatenate DataFrames horizontally\n",
    "    final_df = pd.concat([\n",
    "        url_features_df,\n",
    "        df[['label']]\n",
    "    ], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "print(\"Batch processing function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MAIN FEATURE ENGINEERING PROCESS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING FEATURE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load dataset\n",
    "print(\"\\nLoading PhreshPhish dataset...\")\n",
    "dataset = load_dataset(\"phreshphish/phreshphish\", cache_dir='E:/.cache/huggingface')\n",
    "\n",
    "print(f\"Dataset loaded!\")\n",
    "print(f\"   Train: {len(dataset['train']):,} samples\")\n",
    "print(f\"   Test: {len(dataset['test']):,} samples\")\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 10000  # Process 10K rows at a time\n",
    "\n",
    "# Output CSV file paths (SEPARATE FILES!)\n",
    "TRAIN_OUTPUT_FILE = '../../../data/processed/url-detection/phishing_features_train.csv'\n",
    "TEST_OUTPUT_FILE = '../../../data/processed/url-detection/phishing_features_test.csv'\n",
    "\n",
    "# ============================================\n",
    "# PROCESS TRAIN SET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING TRAIN SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_train_batches = (len(dataset['train']) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in tqdm(range(num_train_batches), desc=\"Processing train batches\"):\n",
    "    # Get batch indices\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(dataset['train']))\n",
    "    \n",
    "    # Select batch\n",
    "    batch = dataset['train'].select(range(start_idx, end_idx))\n",
    "    \n",
    "    # Convert to dict (columns as keys)\n",
    "    batch_dict = {\n",
    "        'url': batch['url'],\n",
    "        'label': batch['label']\n",
    "    }\n",
    "    \n",
    "    # Process batch\n",
    "    processed_batch = process_batch(batch_dict)\n",
    "    \n",
    "    # Save incrementally to TRAIN file\n",
    "    if i == 0:\n",
    "        # First batch: create new CSV with header\n",
    "        processed_batch.to_csv(TRAIN_OUTPUT_FILE, index=False, mode='w')\n",
    "    else:\n",
    "        # Subsequent batches: append without header\n",
    "        processed_batch.to_csv(TRAIN_OUTPUT_FILE, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Train set processed and saved to {TRAIN_OUTPUT_FILE}!\")\n",
    "\n",
    "# ============================================\n",
    "# PROCESS TEST SET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_test_batches = (len(dataset['test']) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in tqdm(range(num_test_batches), desc=\"Processing test batches\"):\n",
    "    # Get batch indices\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(dataset['test']))\n",
    "    \n",
    "    # Select batch\n",
    "    batch = dataset['test'].select(range(start_idx, end_idx))\n",
    "    \n",
    "    # Convert to dict\n",
    "    batch_dict = {\n",
    "        'url': batch['url'],\n",
    "        'label': batch['label']\n",
    "    }\n",
    "    \n",
    "    # Process batch\n",
    "    processed_batch = process_batch(batch_dict)\n",
    "    \n",
    "    # Save incrementally to TEST file\n",
    "    if i == 0:\n",
    "        # First batch: create new CSV with header\n",
    "        processed_batch.to_csv(TEST_OUTPUT_FILE, index=False, mode='w')\n",
    "    else:\n",
    "        # Subsequent batches: append without header\n",
    "        processed_batch.to_csv(TEST_OUTPUT_FILE, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Test set processed and saved to {TEST_OUTPUT_FILE}!\")\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train file: {TRAIN_OUTPUT_FILE}\")\n",
    "print(f\"   - Rows: {len(dataset['train']):,}\")\n",
    "print(f\"   - Columns: 21\")\n",
    "print(f\"\\nTest file: {TEST_OUTPUT_FILE}\")\n",
    "print(f\"   - Rows: {len(dataset['test']):,}\")\n",
    "print(f\"   - Columns: 21\")\n",
    "print(f\"\\nReady for model training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
