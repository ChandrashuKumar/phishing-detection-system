{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca7b543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHISHING DETECTION - FEATURE ENGINEERING\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# FEATURE ENGINEERING - URL FEATURE EXTRACTION WITH TF-IDF\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import pickle\n",
    "\n",
    "os.environ['HF_HOME'] = 'E:/.cache/huggingface'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHISHING DETECTION - FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4829d4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ URL feature extraction functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# URL FEATURE EXTRACTION FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def extract_url_features(url):\n",
    "    \"\"\"\n",
    "    Extract 20 features from a URL\n",
    "    \n",
    "    WHAT: Takes a URL string and extracts numerical features\n",
    "    WHY: Machine learning models need numbers, not text\n",
    "    HOW: Parse the URL and count/check various patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize feature dictionary\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Parse URL into components\n",
    "        # WHAT: urlparse breaks URL into parts (scheme, domain, path, etc.)\n",
    "        # WHY: Easier to analyze individual parts\n",
    "        parsed = urlparse(url)\n",
    "        \n",
    "        # Extract components\n",
    "        scheme = parsed.scheme          # http or https\n",
    "        netloc = parsed.netloc          # domain + subdomain\n",
    "        path = parsed.path              # /path/to/page\n",
    "        params = parsed.params          # parameters\n",
    "        query = parsed.query            # ?key=value\n",
    "        \n",
    "        # ========================================\n",
    "        # CATEGORY 1: BASIC LENGTH FEATURES (4)\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature 1: Total URL Length\n",
    "        # WHAT: Count all characters in URL\n",
    "        # WHY: Phishing URLs are often longer to hide intent\n",
    "        # EXAMPLE: \"https://bit.ly/abc\" = 19 chars\n",
    "        features['url_length'] = len(url)\n",
    "        \n",
    "        # Feature 2: Domain Length\n",
    "        # WHAT: Length of domain name only (without subdomain)\n",
    "        # WHY: Legit domains are short and memorable (google.com, amazon.com)\n",
    "        # HOW: Split domain by dots, take last 2 parts (domain + TLD)\n",
    "        domain_parts = netloc.split('.')\n",
    "        if len(domain_parts) >= 2:\n",
    "            domain = domain_parts[-2] + '.' + domain_parts[-1]\n",
    "            features['domain_length'] = len(domain)\n",
    "        else:\n",
    "            features['domain_length'] = len(netloc)\n",
    "        \n",
    "        # Feature 3: Path Length\n",
    "        # WHAT: Length of path after domain\n",
    "        # WHY: Long paths like /verify/account/login/secure can be suspicious\n",
    "        # EXAMPLE: \"/login\" = 6, \"/a/b/c/d/e\" = 9\n",
    "        features['path_length'] = len(path)\n",
    "        \n",
    "        # Feature 4: TLD (Top-Level Domain) Length\n",
    "        # WHAT: Length of the domain extension (.com, .online, .tk)\n",
    "        # WHY: Suspicious TLDs are often longer\n",
    "        # EXAMPLE: .com = 3, .online = 6\n",
    "        tld = domain_parts[-1] if domain_parts else ''\n",
    "        features['tld_length'] = len(tld)\n",
    "        \n",
    "        # ========================================\n",
    "        # CATEGORY 2: CHARACTER COUNT FEATURES (7)\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature 5: Number of Dots\n",
    "        # WHAT: Count all '.' in URL\n",
    "        # WHY: More dots = more subdomains = potentially suspicious\n",
    "        # EXAMPLE: \"a.b.c.example.com\" = 4 dots\n",
    "        features['num_dots'] = url.count('.')\n",
    "        \n",
    "        # Feature 6: Number of Hyphens\n",
    "        # WHAT: Count all '-' in URL\n",
    "        # WHY: Phishers use hyphens to mimic brands (pay-pal instead of paypal)\n",
    "        # EXAMPLE: \"secure-login-verify.com\" = 2 hyphens\n",
    "        features['num_hyphens'] = url.count('-')\n",
    "        \n",
    "        # Feature 7: Number of Underscores\n",
    "        # WHAT: Count all '_' in URL\n",
    "        # WHY: Rare in legitimate URLs, more common in phishing\n",
    "        # EXAMPLE: \"fake_login_page.com\" = 2 underscores\n",
    "        features['num_underscores'] = url.count('_')\n",
    "        \n",
    "        # Feature 8: Number of Slashes\n",
    "        # WHAT: Count all '/' in URL\n",
    "        # WHY: Deep directory structures can hide malicious pages\n",
    "        # EXAMPLE: \"/a/b/c/d\" = 4 slashes\n",
    "        features['num_slashes'] = url.count('/')\n",
    "        \n",
    "        # Feature 9: Number of Question Marks\n",
    "        # WHAT: Count all '?' in URL\n",
    "        # WHY: Multiple question marks are unusual/suspicious\n",
    "        # EXAMPLE: \"?redirect=?url=\" = 2 question marks\n",
    "        features['num_question'] = url.count('?')\n",
    "        \n",
    "        # Feature 10: Number of Ampersands\n",
    "        # WHAT: Count all '&' in URL\n",
    "        # WHY: Many parameters can indicate tracking or redirection\n",
    "        # EXAMPLE: \"?a=1&b=2&c=3\" = 2 ampersands\n",
    "        features['num_ampersand'] = url.count('&')\n",
    "        \n",
    "        # Feature 11: Number of @ Symbols\n",
    "        # WHAT: Count all '@' in URL\n",
    "        # WHY: MAJOR RED FLAG! Used to trick users\n",
    "        # EXAMPLE: \"http://paypal.com@attacker.com\" redirects to attacker.com\n",
    "        # The @ symbol makes browser ignore everything before it\n",
    "        features['num_at'] = url.count('@')\n",
    "        \n",
    "        # ========================================\n",
    "        # CATEGORY 3: SUSPICIOUS PATTERNS (5)\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature 12: Has IP Address Instead of Domain\n",
    "        # WHAT: Check if domain is an IP address (192.168.1.1)\n",
    "        # WHY: Legitimate sites use domain names, not raw IPs\n",
    "        # HOW: Use regex to match IP pattern\n",
    "        ip_pattern = r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'\n",
    "        features['has_ip_address'] = 1 if re.search(ip_pattern, netloc) else 0\n",
    "        \n",
    "        # Feature 13: Is HTTPS (Secure)\n",
    "        # WHAT: Check if URL uses HTTPS protocol\n",
    "        # WHY: Most legitimate sites use HTTPS in 2024\n",
    "        # HOW: Check scheme\n",
    "        features['is_https'] = 1 if scheme == 'https' else 0\n",
    "        \n",
    "        # Feature 14: Number of Subdomains\n",
    "        # WHAT: Count how many subdomains exist\n",
    "        # WHY: Too many subdomains can be suspicious\n",
    "        # EXAMPLE: \"a.b.c.example.com\" = 3 subdomains (a, b, c)\n",
    "        # HOW: Count domain parts minus 2 (domain + TLD)\n",
    "        features['num_subdomains'] = max(0, len(domain_parts) - 2)\n",
    "        \n",
    "        # Feature 15: Has Non-Standard Port\n",
    "        # WHAT: Check if URL specifies a port number\n",
    "        # WHY: Standard ports (80, 443) are usually omitted\n",
    "        # EXAMPLE: \"example.com:8080\" has port, \"example.com\" doesn't\n",
    "        # Better version\n",
    "        # IPv6 address - check if there's a colon AFTER the closing bracket\n",
    "        if netloc.startswith('['):\n",
    "            features['has_port'] = 1 if ']:' in netloc else 0\n",
    "        # Regular domain - check for colon\n",
    "        else:\n",
    "            features['has_port'] = 1 if ':' in netloc else 0\n",
    "        \n",
    "        # Feature 16: Number of Digits in Domain\n",
    "        # WHAT: Count numerical digits in domain name\n",
    "        # WHY: Legitimate brands rarely use numbers (except l33t sp34k phishing)\n",
    "        # EXAMPLE: \"paypa1.com\" (1 instead of l) = typosquatting\n",
    "        features['num_digits_domain'] = sum(c.isdigit() for c in netloc)\n",
    "        \n",
    "        # ========================================\n",
    "        # CATEGORY 4: SPECIAL CHECKS (4)\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature 17: Suspicious TLD\n",
    "        # WHAT: Check if TLD is commonly used for phishing\n",
    "        # WHY: Free TLDs (.tk, .ml, .ga) are heavily abused\n",
    "        # LIST: Based on research of phishing-prone TLDs\n",
    "        suspicious_tlds = [\n",
    "            'tk', 'ml', 'ga', 'cf', 'gq',  # Free Freenom TLDs\n",
    "            'zip', 'review', 'country', 'kim', 'science',\n",
    "            'work', 'party', 'gdn', 'link'\n",
    "        ]\n",
    "        features['suspicious_tld'] = 1 if tld.lower() in suspicious_tlds else 0\n",
    "        \n",
    "        # Feature 18: Uses Free Hosting\n",
    "        # WHAT: Check if URL uses free hosting service\n",
    "        # WHY: Phishers love free platforms (no cost, easy setup)\n",
    "        # LIST: Common free hosting services\n",
    "        free_hosting_indicators = [\n",
    "            'github.io', 'gitlab.io', 'bitbucket.io',\n",
    "            'netlify.app', 'vercel.app', 'web.app',\n",
    "            'firebaseapp.com', 'herokuapp.com',\n",
    "            'wordpress.com', 'wixsite.com', 'weebly.com',\n",
    "            'blogspot.com', 'tumblr.com',\n",
    "            'pages.dev', 'azurewebsites.net',\n",
    "            '000webhostapp.com', 'freehosting.com',\n",
    "            'googlepages.com', 'gitbook.io'\n",
    "        ]\n",
    "        features['free_hosting'] = 1 if any(indicator in netloc.lower() for indicator in free_hosting_indicators) else 0\n",
    "        \n",
    "        # Feature 19: URL Entropy (Randomness)\n",
    "        # WHAT: Calculate Shannon entropy (measure of randomness)\n",
    "        # WHY: Random strings (asdk2j3k.com) have high entropy\n",
    "        #      Real words (amazon.com) have lower entropy\n",
    "        # HOW: Calculate probability distribution of characters\n",
    "        # MATH: Entropy = -Σ(p(x) * log2(p(x)))\n",
    "        def calculate_entropy(text):\n",
    "            if not text:\n",
    "                return 0\n",
    "            # Count character frequencies\n",
    "            counter = Counter(text)\n",
    "            length = len(text)\n",
    "            # Calculate entropy\n",
    "            entropy = 0\n",
    "            for count in counter.values():\n",
    "                probability = count / length\n",
    "                if probability > 0:\n",
    "                    entropy -= probability * math.log2(probability)\n",
    "            return entropy\n",
    "        \n",
    "        features['url_entropy'] = calculate_entropy(url)\n",
    "        \n",
    "        # Feature 20: Has WWW Prefix\n",
    "        # WHAT: Check if domain starts with 'www.'\n",
    "        # WHY: Many legitimate sites use www, phishing sometimes skips it\n",
    "        # NOTE: Not a strong signal alone, but useful in combination\n",
    "        features['has_www'] = 1 if netloc.startswith('www.') else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If URL parsing fails, return zeros for all features\n",
    "        # WHAT: Handle malformed URLs gracefully\n",
    "        # WHY: Some URLs might be invalid/corrupted\n",
    "        print(f\"Error parsing URL: {url[:50]}... Error: {str(e)}\")\n",
    "        features = {\n",
    "            'url_length': 0, 'domain_length': 0, 'path_length': 0, 'tld_length': 0,\n",
    "            'num_dots': 0, 'num_hyphens': 0, 'num_underscores': 0, 'num_slashes': 0,\n",
    "            'num_question': 0, 'num_ampersand': 0, 'num_at': 0,\n",
    "            'has_ip_address': 0, 'is_https': 0, 'num_subdomains': 0, 'has_port': 0,\n",
    "            'num_digits_domain': 0, 'suspicious_tld': 0, 'free_hosting': 0,\n",
    "            'url_entropy': 0, 'has_www': 0\n",
    "        }\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"✅ URL feature extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4daa7ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PhreshPhish dataset...\n",
      "Train: 371,941 samples\n",
      "Test: 36,787 samples\n",
      "\n",
      "Fitting TF-IDF vectorizer on training URLs...\n",
      "TF-IDF vocabulary size: 500\n",
      "TF-IDF vectorizer saved to ../../../models/url-detection/tfidf_vectorizer.pkl\n",
      "\n",
      "============================================================\n",
      "PROCESSING TRAIN SET\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches: 100%|██████████| 38/38 [02:11<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set saved to ../../../data/processed/url-detection/phishing_features_train.csv\n",
      "\n",
      "============================================================\n",
      "PROCESSING TEST SET\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test batches: 100%|██████████| 4/4 [00:13<00:00,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set saved to ../../../data/processed/url-detection/phishing_features_test.csv\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "============================================================\n",
      "Train file: ../../../data/processed/url-detection/phishing_features_train.csv\n",
      "  Rows: 371,941\n",
      "  Columns: 521 (20 manual + 500 TF-IDF + 1 label)\n",
      "\n",
      "Test file: ../../../data/processed/url-detection/phishing_features_test.csv\n",
      "  Rows: 36,787\n",
      "  Columns: 521\n",
      "\n",
      "TF-IDF vectorizer: ../../../models/url-detection/tfidf_vectorizer.pkl\n",
      "\n",
      "Ready for model training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD DATASET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nLoading PhreshPhish dataset...\")\n",
    "dataset = load_dataset(\"phreshphish/phreshphish\", cache_dir='E:/.cache/huggingface')\n",
    "\n",
    "print(f\"Train: {len(dataset['train']):,} samples\")\n",
    "print(f\"Test: {len(dataset['test']):,} samples\")\n",
    "\n",
    "# ============================================\n",
    "# FIT TF-IDF VECTORIZER ON TRAINING URLS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nFitting TF-IDF vectorizer on training URLs...\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=500,\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "train_urls = [dataset['train'][i]['url'] for i in range(len(dataset['train']))]\n",
    "tfidf_vectorizer.fit(train_urls)\n",
    "\n",
    "print(f\"TF-IDF vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Save vectorizer for later use\n",
    "vectorizer_path = '../../../models/url-detection/tfidf_vectorizer.pkl'\n",
    "os.makedirs(os.path.dirname(vectorizer_path), exist_ok=True)\n",
    "with open(vectorizer_path, 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "print(f\"TF-IDF vectorizer saved to {vectorizer_path}\")\n",
    "\n",
    "# ============================================\n",
    "# BATCH PROCESSING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def process_batch_with_tfidf(batch_data, vectorizer):\n",
    "    df = pd.DataFrame(batch_data)\n",
    "    \n",
    "    # Extract manual URL features\n",
    "    url_features_list = [extract_url_features(url) for url in df['url']]\n",
    "    url_features_df = pd.DataFrame(url_features_list)\n",
    "    \n",
    "    # Extract TF-IDF features\n",
    "    tfidf_features = vectorizer.transform(df['url'])\n",
    "    \n",
    "    # Combine manual features (dense) with TF-IDF (sparse)\n",
    "    manual_features_sparse = csr_matrix(url_features_df.values)\n",
    "    combined_features = hstack([manual_features_sparse, tfidf_features])\n",
    "    \n",
    "    # Create column names\n",
    "    manual_cols = url_features_df.columns.tolist()\n",
    "    tfidf_cols = [f'tfidf_{i}' for i in range(tfidf_features.shape[1])]\n",
    "    all_cols = manual_cols + tfidf_cols\n",
    "    \n",
    "    # Convert to dense DataFrame\n",
    "    combined_df = pd.DataFrame(combined_features.toarray(), columns=all_cols)\n",
    "    combined_df['label'] = df['label'].values\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# ============================================\n",
    "# PROCESS TRAIN SET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING TRAIN SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "TRAIN_OUTPUT_FILE = '../../../data/processed/url-detection/phishing_features_train.csv'\n",
    "\n",
    "num_train_batches = (len(dataset['train']) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in tqdm(range(num_train_batches), desc=\"Processing train batches\"):\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(dataset['train']))\n",
    "    \n",
    "    batch = dataset['train'].select(range(start_idx, end_idx))\n",
    "    batch_dict = {\n",
    "        'url': batch['url'],\n",
    "        'label': batch['label']\n",
    "    }\n",
    "    \n",
    "    processed_batch = process_batch_with_tfidf(batch_dict, tfidf_vectorizer)\n",
    "    \n",
    "    if i == 0:\n",
    "        processed_batch.to_csv(TRAIN_OUTPUT_FILE, index=False, mode='w')\n",
    "    else:\n",
    "        processed_batch.to_csv(TRAIN_OUTPUT_FILE, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Train set saved to {TRAIN_OUTPUT_FILE}\")\n",
    "\n",
    "# ============================================\n",
    "# PROCESS TEST SET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "TEST_OUTPUT_FILE = '../../../data/processed/url-detection/phishing_features_test.csv'\n",
    "\n",
    "num_test_batches = (len(dataset['test']) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in tqdm(range(num_test_batches), desc=\"Processing test batches\"):\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(dataset['test']))\n",
    "    \n",
    "    batch = dataset['test'].select(range(start_idx, end_idx))\n",
    "    batch_dict = {\n",
    "        'url': batch['url'],\n",
    "        'label': batch['label']\n",
    "    }\n",
    "    \n",
    "    processed_batch = process_batch_with_tfidf(batch_dict, tfidf_vectorizer)\n",
    "    \n",
    "    if i == 0:\n",
    "        processed_batch.to_csv(TEST_OUTPUT_FILE, index=False, mode='w')\n",
    "    else:\n",
    "        processed_batch.to_csv(TEST_OUTPUT_FILE, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Test set saved to {TEST_OUTPUT_FILE}\")\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train file: {TRAIN_OUTPUT_FILE}\")\n",
    "print(f\"  Rows: {len(dataset['train']):,}\")\n",
    "print(f\"  Columns: {20 + 500 + 1} (20 manual + 500 TF-IDF + 1 label)\")\n",
    "print(f\"\\nTest file: {TEST_OUTPUT_FILE}\")\n",
    "print(f\"  Rows: {len(dataset['test']):,}\")\n",
    "print(f\"  Columns: {20 + 500 + 1}\")\n",
    "print(f\"\\nTF-IDF vectorizer: {vectorizer_path}\")\n",
    "print(\"\\nReady for model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "734df02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch processing function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# BATCH PROCESSING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def process_batch(batch_data):\n",
    "    \"\"\"\n",
    "    Process a batch of data and extract all features\n",
    "    \n",
    "    WHAT: Takes a batch of rows, extracts features, returns DataFrame\n",
    "    WHY: Can't load 408K rows at once (memory crash)\n",
    "    HOW: Process 10K rows at a time\n",
    "    \n",
    "    Input: Dictionary with columns from dataset\n",
    "    Output: DataFrame with extracted features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert batch to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(batch_data)\n",
    "    \n",
    "    # Extract URL features for each row\n",
    "    # WHAT: Apply feature extraction to every URL\n",
    "    # HOW: Use list comprehension to process all rows\n",
    "    print(\"   Extracting URL features...\")\n",
    "    url_features_list = [extract_url_features(url) for url in df['url']]\n",
    "    url_features_df = pd.DataFrame(url_features_list)\n",
    "    \n",
    "    # Keep label (target variable)\n",
    "    df['label'] = df['label']\n",
    "    \n",
    "    # Combine all features\n",
    "    # WHAT: Merge URL features with label\n",
    "    # HOW: Concatenate DataFrames horizontally\n",
    "    final_df = pd.concat([\n",
    "        url_features_df,\n",
    "        df[['label']]\n",
    "    ], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "print(\"✅ Batch processing function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b007731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING FEATURE EXTRACTION\n",
      "============================================================\n",
      "\n",
      "📥 Loading PhreshPhish dataset...\n",
      "✅ Dataset loaded!\n",
      "   Train: 371,941 samples\n",
      "   Test: 36,787 samples\n",
      "\n",
      "============================================================\n",
      "PROCESSING TRAIN SET\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:   3%|▎         | 1/38 [00:00<00:28,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:   5%|▌         | 2/38 [00:01<00:25,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:   8%|▊         | 3/38 [00:02<00:23,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  11%|█         | 4/38 [00:02<00:22,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  13%|█▎        | 5/38 [00:03<00:21,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  16%|█▌        | 6/38 [00:03<00:20,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  18%|█▊        | 7/38 [00:04<00:19,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  21%|██        | 8/38 [00:05<00:18,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  24%|██▎       | 9/38 [00:05<00:17,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  26%|██▋       | 10/38 [00:06<00:17,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  29%|██▉       | 11/38 [00:06<00:16,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  32%|███▏      | 12/38 [00:07<00:16,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  34%|███▍      | 13/38 [00:08<00:15,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  37%|███▋      | 14/38 [00:08<00:14,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  39%|███▉      | 15/38 [00:09<00:14,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  42%|████▏     | 16/38 [00:10<00:14,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  45%|████▍     | 17/38 [00:11<00:14,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  47%|████▋     | 18/38 [00:11<00:14,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  50%|█████     | 19/38 [00:12<00:13,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  53%|█████▎    | 20/38 [00:13<00:12,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  55%|█████▌    | 21/38 [00:13<00:11,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  58%|█████▊    | 22/38 [00:14<00:10,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  61%|██████    | 23/38 [00:15<00:10,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  63%|██████▎   | 24/38 [00:15<00:09,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  66%|██████▌   | 25/38 [00:16<00:08,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  68%|██████▊   | 26/38 [00:17<00:07,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  71%|███████   | 27/38 [00:17<00:07,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  74%|███████▎  | 28/38 [00:18<00:06,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  76%|███████▋  | 29/38 [00:18<00:05,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  79%|███████▉  | 30/38 [00:19<00:04,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  82%|████████▏ | 31/38 [00:20<00:04,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  84%|████████▍ | 32/38 [00:20<00:03,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  87%|████████▋ | 33/38 [00:21<00:03,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  89%|████████▉ | 34/38 [00:21<00:02,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  92%|█████████▏| 35/38 [00:22<00:01,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:  95%|█████████▍| 36/38 [00:23<00:01,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches: 100%|██████████| 38/38 [00:23<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n",
      "✅ Train set processed and saved to ../../../data/processed/url-detection/phishing_features_train.csv!\n",
      "\n",
      "============================================================\n",
      "PROCESSING TEST SET\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test batches:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test batches:  25%|██▌       | 1/4 [00:00<00:01,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test batches:  50%|█████     | 2/4 [00:01<00:01,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test batches: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracting URL features...\n",
      "✅ Test set processed and saved to ../../../data/processed/url-detection/phishing_features_test.csv!\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING COMPLETE!\n",
      "============================================================\n",
      "📁 Train file: ../../../data/processed/url-detection/phishing_features_train.csv\n",
      "   - Rows: 371,941\n",
      "   - Columns: 21\n",
      "\n",
      "📁 Test file: ../../../data/processed/url-detection/phishing_features_test.csv\n",
      "   - Rows: 36,787\n",
      "   - Columns: 21\n",
      "\n",
      "✅ Ready for model training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MAIN FEATURE ENGINEERING PROCESS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING FEATURE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load dataset\n",
    "print(\"\\n📥 Loading PhreshPhish dataset...\")\n",
    "dataset = load_dataset(\"phreshphish/phreshphish\", cache_dir='E:/.cache/huggingface')\n",
    "\n",
    "print(f\"✅ Dataset loaded!\")\n",
    "print(f\"   Train: {len(dataset['train']):,} samples\")\n",
    "print(f\"   Test: {len(dataset['test']):,} samples\")\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 10000  # Process 10K rows at a time\n",
    "\n",
    "# Output CSV file paths (SEPARATE FILES!)\n",
    "TRAIN_OUTPUT_FILE = '../../../data/processed/url-detection/phishing_features_train.csv'\n",
    "TEST_OUTPUT_FILE = '../../../data/processed/url-detection/phishing_features_test.csv'\n",
    "\n",
    "# ============================================\n",
    "# PROCESS TRAIN SET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING TRAIN SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_train_batches = (len(dataset['train']) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in tqdm(range(num_train_batches), desc=\"Processing train batches\"):\n",
    "    # Get batch indices\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(dataset['train']))\n",
    "    \n",
    "    # Select batch\n",
    "    batch = dataset['train'].select(range(start_idx, end_idx))\n",
    "    \n",
    "    # Convert to dict (columns as keys)\n",
    "    batch_dict = {\n",
    "        'url': batch['url'],\n",
    "        'label': batch['label']\n",
    "    }\n",
    "    \n",
    "    # Process batch\n",
    "    processed_batch = process_batch(batch_dict)\n",
    "    \n",
    "    # Save incrementally to TRAIN file\n",
    "    if i == 0:\n",
    "        # First batch: create new CSV with header\n",
    "        processed_batch.to_csv(TRAIN_OUTPUT_FILE, index=False, mode='w')\n",
    "    else:\n",
    "        # Subsequent batches: append without header\n",
    "        processed_batch.to_csv(TRAIN_OUTPUT_FILE, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"✅ Train set processed and saved to {TRAIN_OUTPUT_FILE}!\")\n",
    "\n",
    "# ============================================\n",
    "# PROCESS TEST SET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_test_batches = (len(dataset['test']) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in tqdm(range(num_test_batches), desc=\"Processing test batches\"):\n",
    "    # Get batch indices\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(dataset['test']))\n",
    "    \n",
    "    # Select batch\n",
    "    batch = dataset['test'].select(range(start_idx, end_idx))\n",
    "    \n",
    "    # Convert to dict\n",
    "    batch_dict = {\n",
    "        'url': batch['url'],\n",
    "        'label': batch['label']\n",
    "    }\n",
    "    \n",
    "    # Process batch\n",
    "    processed_batch = process_batch(batch_dict)\n",
    "    \n",
    "    # Save incrementally to TEST file\n",
    "    if i == 0:\n",
    "        # First batch: create new CSV with header\n",
    "        processed_batch.to_csv(TEST_OUTPUT_FILE, index=False, mode='w')\n",
    "    else:\n",
    "        # Subsequent batches: append without header\n",
    "        processed_batch.to_csv(TEST_OUTPUT_FILE, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"✅ Test set processed and saved to {TEST_OUTPUT_FILE}!\")\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📁 Train file: {TRAIN_OUTPUT_FILE}\")\n",
    "print(f\"   - Rows: {len(dataset['train']):,}\")\n",
    "print(f\"   - Columns: 21\")\n",
    "print(f\"\\n📁 Test file: {TEST_OUTPUT_FILE}\")\n",
    "print(f\"   - Rows: {len(dataset['test']):,}\")\n",
    "print(f\"   - Columns: 21\")\n",
    "print(f\"\\n✅ Ready for model training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
